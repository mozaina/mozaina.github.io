<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Spark从入门到精通二spark任务的提交方式sparkshellsparksubmit各种版本的wordcount | 开发者问答集锦</title>
    <meta property="og:title" content="Spark从入门到精通二spark任务的提交方式sparkshellsparksubmit各种版本的wordcount - 开发者问答集锦">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2020-09-02T00:00:00&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2020-09-02T00:00:00&#43;08:00'>
        
    <meta name="Keywords" content="">
    <meta name="description" content="Spark从入门到精通二spark任务的提交方式sparkshellsparksubmit各种版本的wordcount">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/posts/spark%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%E4%BA%8Cspark%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8Fsparkshellsparksubmit%E5%90%84%E7%A7%8D%E7%89%88%E6%9C%AC%E7%9A%84wordcount/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">Spark从入门到精通二spark任务的提交方式sparkshellsparksubmit各种版本的wordcount</h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            

<p>版权声明：本文为博主原创文章，未经博主允许不得转载!!</p>

<p>欢迎访问:<a href="https://blog.csdn.net/qq_21439395/article/details/82779266">https://blog.csdn.net/qq_21439395/article/details/82779266</a></p>

<p>交流QQ: 824203453</p>

<ol>
<li>执行Spark程序</li>
</ol>

<p>使用spark-shell命令和spark-submit命令来提交spark任务。</p>

<p>当执行测试程序，使用spark-shell，spark的交互式命令行</p>

<p>提交spark程序到spark集群中运行时，spark-submit</p>

<ol>
<li>1. 执行第一个spark示例程序</li>
</ol>

<p>spark-submit &ndash;class org.apache.spark.examples.SparkPi
/root/apps/spark/examples/jars/spark-examples_2.11-2.2.0.jar 100</p>

<p>该算法是利用蒙特·卡罗算法求PI(圆周率)</p>

<p>spark任务提交的方式：</p>

<p>spark-submit &ndash;master spark://hdp-01:7077 &ndash;class xxx.SparkPi /root/xx.jar
输入输出参数</p>

<p>怎么用：</p>

<p>提交正式任务，或者有jar包，使用spark-submit ；本地测试，选用spark-shell</p>

<ol>
<li>1. 启动Spark Shell</li>
</ol>

<p>spark-shell 用命令行的方式提交任务到集群的一个客户端。spark-
shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。</p>

<ol>
<li>1.       1. 启动spark shell</li>
</ol>

<p>直接启动spark-shell默认使用的是local模式，和spark集群无关</p>

<p>只要把spark安装包解压了，就可以运行local模式</p>

<p><a href="https://img.it610.com/image/info8/9a1242ebcf1c47a89bd2ed7ad177f1cc.jpg"><img src="https://img.it610.com/image/info8/9a1242ebcf1c47a89bd2ed7ad177f1cc.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第1张图片" /></a></p>

<p>local模式没有指定master地址，仅在本机启动一个进程（SparkSubmit），没有与集群建立联系。但是也可以正常启动spark
shell和执行spark shell中的程序</p>

<p>指定集群模式启动：</p>

<p>hdfs://hdp-01:9000</p>

<p>spark的协议URI： <strong>spark://hdp-01:7077</strong></p>

<h1 id="spark-shell-master">spark-shell &ndash;master</h1>

<p><a href="https://img.it610.com/image/info8/db3538415df04bdbb23399ab6e16e20c.jpg"><img src="https://img.it610.com/image/info8/db3538415df04bdbb23399ab6e16e20c.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第2张图片" /></a></p>

<p>在webUI界面，可以查看到正在运行的程序：</p>

<p><a href="https://img.it610.com/image/info8/78b212d027634260853beca51957ca7a.png"><img src="https://img.it610.com/image/info8/78b212d027634260853beca51957ca7a.png" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第3张图片" /></a></p>

<p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可</p>

<ol>
<li>1.       1. 在spark shell中编写WordCount程序</li>
<li>首先启动hdfs</li>
<li>向hdfs上传一个文件到hdfs://hdp-01:9000/wordcount/input/a.txt</li>
<li>在spark shell中用scala语言编写spark程序</li>
</ol>

<p>scala&gt; sc.textFile(&ldquo;hdfs://hdp-01:9000/wordcount/input/&ldquo;)</p>

<p>spark是懒加载的，所以这里并没有真正执行任务。可使用collect方法快速查看数据。</p>

<p>lazy执行的，只有调用了action方法，才正式开始运行。</p>

<p>scala&gt;sc.textFile(&ldquo;hdfs://hdp-01:9000/wordcount/input/&ldquo;).flatMap(<em>.split(&rdquo;
&ldquo;)).map((</em>,1)).reduceByKey(_ + <em>).sortBy(</em>._2,false).collect</p>

<p>注意：这些flatMap，map等方法是RDD上的方法，要区分于原生的scala方法。</p>

<p>和原生scala的方法名称有的相同，但属于不通的类的方法，底层实现完全不一致。</p>

<p>原生的方法： 对单机的数组或集合进行操作。</p>

<p>RDD上的方法：</p>

<p>RDD是spark的计算模型，RDD上有很多的方法，这些方法通常称为算子，主要有两类算子，一类是transform，一类是action，transform是懒加载的。</p>

<p>scala&gt;sc.textFile(&ldquo;hdfs://hdp-01:9000/wordcount/input/&ldquo;).flatMap(<em>.split(&rdquo;
&ldquo;)).map((</em>,1)).reduceByKey(<em>+</em>).saveAsTextFile(&ldquo;hdfs://hdp-01:9000/wordcount/outspark1&rdquo;)</p>

<ol>
<li>使用hdfs命令查看结果</li>
</ol>

<h1 id="hadoop-fs-ls-wordcount-outspark1">hadoop fs -ls /wordcount/outspark1</h1>

<p>说明：</p>

<p>sc是SparkContext对象，该对象是提交spark程序的入口</p>

<p>textFile(hdfs://hdp-01:9000/wordcount/intput/a.txt)是hdfs中读取数据</p>

<p>flatMap(_.split(&rdquo; &ldquo;))先map再压平</p>

<p>map((_,1))将单词和1构成元组</p>

<p>reduceByKey(<em>+</em>)按照key进行reduce，并将value累加</p>

<p>saveAsTextFile(&ldquo;hdfs://hdp-01:9000/outspark1&rdquo;)将结果写入到hdfs中</p>

<p>spark中的方法很多，这些方法统称为算子。一共有两类算子（transform，action）</p>

<p>spark是懒加载的，transform方法并不会立即执行，只有当程序遇到action的时候才会被执行。collect算子是一个action</p>

<p>collect: 收集数据到本地</p>

<ol>
<li>1. 在IDEA中编写WordCount程序</li>
</ol>

<p>spark
shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中开发程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。</p>

<ol>
<li>1.       1. scalaAPI的wordcount</li>
</ol>

<p>1.创建一个项目</p>

<p><a href="https://img.it610.com/image/info8/139459f685984e869752af8e3e3cc3d8.jpg"><img src="https://img.it610.com/image/info8/139459f685984e869752af8e3e3cc3d8.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第4张图片" /></a></p>

<p>2.选择Maven项目，然后点击next</p>

<p><a href="https://img.it610.com/image/info8/4fdb760b4d6f4a01abc308a50025a8cd.jpg"><img src="https://img.it610.com/image/info8/4fdb760b4d6f4a01abc308a50025a8cd.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第5张图片" /></a></p>

<p>3.填写maven的GAV，然后点击next</p>

<p><a href="https://img.it610.com/image/info8/6672b198197147f7847314ffa279c673.jpg"><img src="https://img.it610.com/image/info8/6672b198197147f7847314ffa279c673.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第6张图片" /></a></p>

<ol>
<li>填写项目名称，然后点击finish</li>
</ol>

<p><a href="https://img.it610.com/image/info8/164cfe8eaaa244d0a8a5be2c1aae12d2.jpg"><img src="https://img.it610.com/image/info8/164cfe8eaaa244d0a8a5be2c1aae12d2.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第7张图片" /></a></p>

<p>5.创建好maven项目后，点击Import Changes 手动导入，点击Enable Auto-Import 可自动导入</p>

<p><a href="https://img.it610.com/image/info8/f870614186bd49b2a65f675c23211588.jpg"><img src="https://img.it610.com/image/info8/f870614186bd49b2a65f675c23211588.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第8张图片" /></a></p>

<ol>
<li>配置Maven的pom.xml</li>
</ol>

<p>详见pom.xml文件</p>

<p>maven的编译jdk版本设置：</p>

<p><a href="https://img.it610.com/image/info8/f42e1fd206264387aacb912d32fd7d11.jpg"><img src="https://img.it610.com/image/info8/f42e1fd206264387aacb912d32fd7d11.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第9张图片" /></a></p>

<ol>
<li>新建一个scala object</li>
<li>编写spark程序</li>
</ol>

<p><strong>object</strong> WordCount {<br />
<strong>def</strong> main(args: Array[String]): Unit = {<br />
<strong>if</strong> (args.length!=2){<br />
<em>println</em> ( <strong>&ldquo;cn.edu360.sparkcore.WordCount &ldquo;</strong>)<br />
sys. <em>exit</em> (1)<br />
}<br />
<strong>val</strong> <em>Array</em> (input,output) = args<br />
_/**<br />
* flatMap map _<em>都是</em> <em>rdd</em> _上的方法<br />
_<em>* scala</em> <em>的</em> <em>api</em> <em>中，也有</em> <em>flatMap map</em> _方法<br />
_<em>*</em> <em>仅仅是名称一样而已，一个属于</em> <em>RDD</em> _，一个属于本地集合<br />
_<em>*</em> <em>在操作</em> <em>RDD</em> _的时候，是不是和本地集合一样的。<br />
_<em>*</em> <em>使用</em> <em>spark</em> <em>来运行程序，不需要再指定</em> <em>main</em> _方法了。<br />
__*/<br />
// __配置参数<br />
_<strong>val</strong> conf = <strong>new</strong> SparkConf()<br />
<em>// spark</em> <em>程序执行的入口</em> _SparkContext<br />
_<strong>val</strong> sc: SparkContext = <strong>new</strong> SparkContext(conf)<br />
<em>// 1,</em> _读取文件<br />
_<strong>val</strong> data: RDD[String] = sc.textFile(input)<br />
<em>//</em> _切分<br />
<em><strong>val</strong> lines: RDD[String] = data.flatMap(</em>.split( <strong>&rdquo; &ldquo;</strong> ))<br />
<em>//</em> _组装<br />
<em><strong>val</strong> wordWithOne: RDD[(String, Int)] = lines.map((</em>, 1))<br />
<em>//</em> _分组聚合<br />
<em><strong>val</strong> key: RDD[(String, Int)] = wordWithOne.reduceByKey(</em> + _) _// ((a,b)=
&gt; a+b)<br />
// __可选：排序 倒序排序<br />
_<strong>val</strong> result: RDD[(String, Int)] = key.sortBy(t =&gt; -t._2)<br />
_// key.sortBy(t= &gt; t._2,false)<br />
// _<em>写文件 到</em> <em>hdfs</em> _中<br />
_key.saveAsTextFile(output)</p>

<p><em>//</em> <em>释放资源</em></p>

<p>sc.stop()<br />
}<br />
}</p>

<hr />

<ol>
<li>1.       1. JAVAAPI的wordcount</li>
</ol>

<p><strong>public class</strong> JavaWordCount {<br />
<strong>public static void</strong> main(String[] args) {<br />
<strong>if</strong> (args. <strong>length</strong>!=2){ <em>//</em> _快捷键 sou psvm<br />
_System. ** <em>out</em>**.println( <strong>&ldquo;cn.edu360.sparkcore.JavaWordCount &ldquo;</strong>);<br />
System. <em>exit</em> (1);<br />
}<br />
<em>//spark</em> _程序SparkContext<br />
_SparkConf conf = <strong>new</strong> SparkConf();<br />
JavaSparkContext sc = <strong>new</strong> JavaSparkContext(conf);<br />
<em>//</em> _获取数据<br />
_JavaRDD data = sc.textFile(args[0]);<br />
<em>//</em> _切分 输入 类型 &mdash;》 输出类型<br />
_JavaRDD lines = data.flatMap( <strong>new</strong> FlatMapFunction() {<br />
<em>//</em> _调用每一条数据，进行处理<br />
_@Override<br />
<strong>public</strong> Iterator call(String s) <strong>throws</strong> Exception {<br />
 <em>// s</em> _： hello spark<br />
// 把数据String [] 转换成iterator Arrays.asList().iterator()<br />
_<strong>return</strong> Arrays. <em>asList</em> (s.split( <strong>&rdquo; &ldquo;</strong> )).iterator();<br />
}<br />
});<br />
<em>//</em> _组装 hello &mdash; &gt; (hello,1)<br />
// 3个参数类型，输入数据类型 返回值类型（String，Integer）<br />
_JavaPairRDD wordwithOne = lines.mapToPair( <strong>new</strong> PairFunction() {<br />
@Override<br />
<strong>public</strong> Tuple2 call(String word) <strong>throws</strong> Exception {<br />
<strong>return new</strong> Tuple2&lt;&gt;(word, 1);<br />
}<br />
});</p>

<p><em>//</em> <em>分组聚合 reduceByKey(</em>+_) redeceByKey((a,b)= &gt;a+b) (hello,5)<br />
// 3 个参数类型<br />
_JavaPairRDD resutlt = wordwithOne.reduceByKey( <strong>new</strong> Function2() {<br />
@Override<br />
<strong>public</strong> Integer call(Integer v1, Integer v2) <strong>throws</strong> Exception {<br />
<strong>return</strong> v1 + v2;<br />
}<br />
});<br />
<em>//</em> _排序<br />
/** javaAPi不可以指定排序的规则 sortByKey<br />
* 先把数据k-v互换 ，然后再调用sortByKey的方法，然后再互换回去<br />
*/<br />
_JavaPairRDD swapedResult = resutlt.mapToPair( <strong>new</strong> PairFunction, Integer,
String&gt;() {<br />
@Override<br />
<strong>public</strong> Tuple2 call(Tuple2 tp) <strong>throws</strong> Exception {<br />
 _// new Tuple2 &lt;&gt;(tp._2,tp.<em>1)<br />
// k- v</em> _互换<br />
_<strong>return</strong> tp.swap();<br />
}<br />
});<br />
<em>// sortByKey</em> _默认是升序<br />
_JavaPairRDD sortedResult = swapedResult.sortByKey( <strong>false</strong> );<br />
<em>//</em> _得到使用javaAPI 计算的wordcount<br />
_JavaPairRDD finalResult = sortedResult.mapToPair( <strong>new</strong> PairFunction,
String, Integer&gt;() {<br />
@Override<br />
<strong>public</strong> Tuple2 call(Tuple2 tp) <strong>throws</strong> Exception {<br />
<strong>return</strong> tp.swap();<br />
}<br />
});<br />
<em>//</em> _写文件<br />
_finalResult.saveAsTextFile(args[1]);</p>

<p><em>//</em> <em>释放资源</em></p>

<p>sc.stop();<br />
}<br />
}</p>

<hr />

<ol>
<li>1.       1. JAVALambda的wordcount</li>
</ol>

<p><strong>public class</strong> JavaLambdaWC {<br />
<strong>public static void</strong> main(String[] args) {<br />
<strong>if</strong> (args. <strong>length</strong>!=2){ <em>//</em> <em>快捷键</em> _sou psvm<br />
_System. ** <em>out</em>**.println( <strong>&ldquo;cn.edu360.sparkcore.JavaWordCount &ldquo;</strong>);<br />
System. <em>exit</em> (1);<br />
}<br />
<em>//spark</em> <em>程序</em> _SparkContext<br />
_SparkConf conf = <strong>new</strong> SparkConf();<br />
JavaSparkContext sc = <strong>new</strong> JavaSparkContext(conf);<br />
<em>//</em> _获取数据<br />
_JavaRDD data = sc.textFile(args[0]);<br />
<em>//</em> _分割并压平<br />
_JavaRDD lines = data.flatMap(t -&gt; Arrays. <em>asList</em> (t.split( <strong>&rdquo; &ldquo;</strong>
)).iterator());<br />
<em>//</em> _组装<br />
_JavaPairRDD wordwithOne = lines.mapToPair(t -&gt; <strong>new</strong> Tuple2(t, 1));<br />
<em>//</em> _分组聚合<br />
_JavaPairRDD result = wordwithOne.reduceByKey((a, b) -&gt; a + b);<br />
<em>//</em> <em>排序 先</em> <em>k- v</em> _互换<br />
_JavaPairRDD swapedResult = result.mapToPair(t -&gt; t.swap());<br />
<em>//</em> _再排序<br />
_JavaPairRDD sortedResult = swapedResult.sortByKey( <strong>false</strong> );<br />
JavaPairRDD finalRes = sortedResult.mapToPair(t -&gt; t.swap());<br />
<em>//</em> <em>结果数据写入到</em> <em>hdfs</em> _中<br />
_finalRes.saveAsTextFile(args[1]);<br />
<em>//</em> _释放资源<br />
_sc.stop();<br />
}<br />
}</p>

<hr />

<ol>
<li>1.       1. local模式运行spark程序</li>
</ol>

<p><em>//</em> <em>配置参数</em><br />
 <strong>val</strong> conf = <strong>new</strong> SparkConf()<br />
 <em>//</em> <em>设置</em> <em>master</em> <em>为</em> <em>local</em> <em>模式 本地模式：</em> <em>local local[*] local[2]</em><br />
conf.setMaster( <strong>&ldquo;local[*]&rdquo;</strong> )<br />
conf.setAppName(WordCount.getClass.getSimpleName)</p>

<hr />

<ol>
<li>1. 打包并上传到集群</li>
</ol>

<p>点击idea右侧的Maven Project选项</p>

<p>点击Lifecycle,选择clean和package，然后点击Run Maven Build</p>

<p><a href="https://img.it610.com/image/info8/ec889e5f52754829ab427a26ef071310.jpg"><img src="https://img.it610.com/image/info8/ec889e5f52754829ab427a26ef071310.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第10张图片" /></a></p>

<ol>
<li>选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上（任意节点即可）</li>
</ol>

<p><a href="https://img.it610.com/image/info8/0341ba795f1e43b8be43d4a028c8e8f2.jpg"><img src="https://img.it610.com/image/info8/0341ba795f1e43b8be43d4a028c8e8f2.jpg" alt="Spark从入门到精通二----spark任务的提交方式spark-shell/spark-submit----------
各种版本的wordcount_第11张图片" /></a></p>

<p><a href="https://img.it610.com/image/info8/b3631406eacb45de891b4b7c41673065.jpg"><img src="https://img.it610.com/image/info8/b3631406eacb45de891b4b7c41673065.jpg" alt="" /></a></p>

<p>确保启动了hdfs集群和spark集群</p>

<h1 id="hdfs启动-在namenode节点上">hdfs启动（在namenode节点上）</h1>

<h1 id="root-apps-hadoop-sbin-start-dfs-sh">/root/apps/hadoop/sbin/start-dfs.sh</h1>

<h1 id="spark启动-在master节点上">spark启动（在master节点上）</h1>

<h1 id="start-all-sh">start-all.sh</h1>

<hr />

<ol>
<li>1. 提交任务</li>
</ol>

<p>使用spark-submit命令提交Spark应用（注意参数的顺序）</p>

<p>spark-submit &ndash;master spark://hdp-01:7077 &ndash;class cn.edu360.spark.WordCount
sparkcore-1.0-SNAPSHOT.jar hdfs://hdp-01:9000/wordcount/input
hdfs://hdp-01:9000/wordcount/output</p>

<hr />

<p><a href="https://img.it610.com/image/info8/9bc0a5986c9d4c3eb5b4d8dce314384f.jpg"><img src="https://img.it610.com/image/info8/9bc0a5986c9d4c3eb5b4d8dce314384f.jpg" alt="" /></a></p>

<p>可以分多行写：</p>

<p>spark-submit </p>

<p>--class cn.edu360.spark.WordCount </p>

<p>--master spark://hdp-01:7077 </p>

<p>/root/sparkcore-1.0-SNAPSHOT.jar </p>

<p>hdfs://hdp-01:9000/wordcount/input </p>

<p>hdfs://hdp-01:9000/wordcount/output</p>

<p>任务执行命令的基本套路：</p>

<h1 id="spark-submit-任务提交参数-class-程序的main方法-jar包-main的参数列表">spark-submit 任务提交参数 &ndash;class 程序的main方法 jar包 main的参数列表</h1>

<p>查看程序执行过程：</p>

<p>在web页面查看程序运行状态：<a href="http://hdp-01:8080">http://hdp-01:8080</a></p>

<p>使用jps命令查看进程信息</p>

<p>查看hdfs文件结果</p>

<p>hdfs dfs -cat hdfs://hdp-01:9000/output/part-00000</p>

<p>版权声明：本文为博主原创文章，未经博主允许不得转载!!</p>

<p>欢迎访问:<a href="https://blog.csdn.net/qq_21439395/article/details/82779266">https://blog.csdn.net/qq_21439395/article/details/82779266</a></p>

<p>交流QQ: 824203453</p>

        </div>

        


        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a></li>
        
        <li><a href="/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/">009Shell脚本下条件测试eqne</a></li>
        
        <li><a href="/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/">00Pythonmanagepyshell和Python的分析</a></li>
        
        <li><a href="/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a></li>
        
        <li><a href="/posts/018dockerfileshell/">018DockerfileSHELL</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='https://zaina.newban.cn/tags/shell'>shell</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
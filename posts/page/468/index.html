<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Posts | 开发者问答集锦</title>
    <meta property="og:title" content="Posts - 开发者问答集锦">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="">
    <meta name="description" content="Posts">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/posts/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <link rel="alternate" type="application/rss+xml+xml" href="https://zaina.newban.cn/posts/index.xml" title="开发者问答集锦" />
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksql%E6%95%B4%E5%90%88hive%E5%9C%A8sparksql%E5%91%BD%E4%BB%A4%E5%92%8Csparkshell%E5%91%BD%E4%BB%A4%E4%B8%8B%E6%89%A7%E8%A1%8Csql%E5%91%BD%E4%BB%A4%E5%92%8C%E6%95%B4%E5%90%88%E8%B0%83%E7%94%A8hive/" title="SparkSql整合hive在sparksql命令和sparkshell命令下执行sql命令和整合调用hive">SparkSql整合hive在sparksql命令和sparkshell命令下执行sql命令和整合调用hive</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            1.安装hive
如果想创建一个数据库用户，并且为数据库赋值权限，可以参考：http://blog.csdn.net/tototuzuoquan/article/details/52785504
2.将配置好的hive-site.xml、core-site.xml、hdfs-site.xml放入$SPARK_HOME/conf目录下
[root@hadoop1 conf]# cd /home/tuzq/software/hive/apache-hive-1.2.1-bin [root@hadoop1 conf]# cp hive-site.xml $SPARK_HOME/conf [root@hadoop1 spark-1.6.2-bin-hadoop2.6]# cd $HADOOP_HOME [root@hadoop1 hadoop]# cp core-site.xml $SPARK_HOME/conf [root@hadoop1 hadoop]# cp hdfs-site.xml $SPARK_HOME/conf 同步spark集群中的conf中的配置 [root@hadoop1 conf]# scp -r * root@hadoop2:$PWD [root@hadoop1 conf]# scp -r * root@hadoop3:$PWD [root@hadoop1 conf]# scp -r * root@hadoop4:$PWD [root@hadoop1 conf]# scp -r * root@hadoop5:$PWD  放入进去之后，注意重新启动Spark集群，关于集群启动和停止，可以参考：
http://blog.csdn.net/tototuzuoquan/article/details/74481570  修改spark的log4j打印输出的日志错误级别为Error。修改内容为：

3.启动spark-shell时指定mysql连接驱动位置
bin/spark-shell --master spark://hadoop1:7077,hadoop2:7077 --executor-memory 1g --total-executor-cores 2 --driver-class-path /home/tuzq/software/spark-1.6.2-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar  如果启动的过程中报如下错：……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksql%E6%95%B4%E5%90%88hive%E5%9C%A8sparksql%E5%91%BD%E4%BB%A4%E5%92%8Csparkshell%E5%91%BD%E4%BB%A4%E4%B8%8B%E6%89%A7%E8%A1%8Csql%E5%91%BD%E4%BB%A4%E5%92%8C%E6%95%B4%E5%90%88%E8%B0%83%E7%94%A8hive/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksql%E4%BB%8Emysql%E4%B8%AD%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E5%B0%86%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0mysql%E4%B8%ADsparkshell%E6%96%B9%E5%BC%8Fsparksql%E7%A8%8B%E5%BA%8F/" title="SparkSQL从MySQL中加载数据以及将数据写入到mysql中SparkShell方式SparkSQL程序">SparkSQL从MySQL中加载数据以及将数据写入到mysql中SparkShell方式SparkSQL程序</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            分享一下我老师大神的人工智能教程！零基础，通俗易懂！http://blog.csdn.net/jiangjunshow
也欢迎大家转载本篇文章。分享知识，造福人民，实现我们中华民族伟大复兴！
1． JDBC Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。
1.1． 从MySQL中加载数据（Spark Shell方式） 1.启动Spark Shell，必须指定mysql连接驱动jar包
[root@hadoop1 spark-2.1.1-bin-hadoop2.7]# bin/spark-shell --master spark://hadoop1:7077,hadoop2:7077 --jars /home/tuzq/software/spark-2.1.1-bin-hadoop2.7/jars/mysql-connector-java-5.1.38.jar --driver-class-path /home/tuzq/software/spark-2.1.1-bin-hadoop2.7/jars/mysql-connector-java-5.1.38.jar   1  
2.从mysql中加载数据
进入bigdata中创建person表：
CREATE DATABASE bigdata CHARACTER SET utf8;USE bigdata;CREATE TABLE person ( id INT(10) AUTO_INCREMENT PRIMARY KEY, name varchar(100), age INT(3)) ENGINE=INNODB DEFAULT CHARSET=utf8;   1 2 3 4 5 6 7  并初始化数据：

scala&gt; val sqlContext = new org.apache.spark.sql.SQLContext(sc)   1……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksql%E4%BB%8Emysql%E4%B8%AD%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E5%B0%86%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0mysql%E4%B8%ADsparkshell%E6%96%B9%E5%BC%8Fsparksql%E7%A8%8B%E5%BA%8F/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkspawnaappviasparkshellvssparksubmit/" title="sparkspawnaappviasparkshellVSsparksubmit">sparkspawnaappviasparkshellVSsparksubmit</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            ……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkspawnaappviasparkshellvssparksubmit/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksparkshell%E6%93%8D%E4%BD%9C/" title="Sparksparkshell操作">Sparksparkshell操作</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            RDD(Resilient Distributed Datasets),弹性分布式数据集,是分布式内存的一个抽象概念，RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和group by）而创建，然而这些限制使得实现容错的开销很低.
创建RDD的两种方法:
1.并行化集合
val data= sc.parallelize(Array(1,2,3))
2.外部数据集
val textFile = sc.textFile(&ldquo;file:///opt/word.txt&rdquo;)
map(对所有的元素进行操作)函数:

filter(过滤元素)函数:

count(计算元素个数)函数:

distinct(去重):

union(并集):

intersection(交集):

cartesian(笛卡尔积):

sortByKey(排序)函数:

groupByKey(分组)函数:

reduceByKey数据聚合函数:

cogroup

first(第一个元素):

take(返回前几个元素):

在/opt目录下新建文件word.txt,并输入一些内容。
启动spark-shell
val textFile = sc.textFile(&ldquo;file:///opt/word.txt&rdquo;)
textFile.first()
val wordCount = textFile.flatMap(line =&gt; line.split(&rdquo; &ldquo;)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)
wordCount.collect()

val rr=sc.textFile(&ldquo;file:///opt/word.txt&rdquo;).flatMap(x=&gt;x.split(&rdquo; &ldquo;)).countByValue()

上面是读取本地文件，还可以读取hdfs上面的文件……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksparkshell%E6%93%8D%E4%BD%9C/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksparkshell%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/" title="Sparksparkshell命令使用">Sparksparkshell命令使用</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            环境： 操作系统：CentOS7.3
Java: jdk1.8.0_45
Hadoop：hadoop-2.6.0-cdh5.14.0.tar.gz
1. spark-shell 使用帮助
[hadoop@hadoop01 ~]$ cd app/spark-2.2.0-bin-2.6.0-cdh5.7.0/bin [hadoop@hadoop01 bin]$ ./spark-shell --help Usage: ./bin/spark-shell [options] # 重要参数 Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --conf PROP=VALUE Arbitrary Spark configuration property.……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksparkshell%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E9%80%80%E5%87%BA%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/" title="Sparkshell退出操作以及出现问题的解决方法">Sparkshell退出操作以及出现问题的解决方法</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            启动spark的操作是在其根目录下输入，在终端中输入：
 ./bin/spark-shell  退出的正确操作是：
:quit  然而我们的错误操作是：
Ctrl+C或Z  这样就会在重启的时候报错。
wugaosheng:spark-2.2.0-bin-hadoop2.7 eric$ ./bin/spark-shell Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 17/10/06 17:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 17/10/06 17:15:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041. 17/10/06 17:15:33 ERROR Schema: Failed initialising database.……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E9%80%80%E5%87%BA%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98/" title="sparkshell连接异常问题">sparkshell连接异常问题</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            [root@node00 sbin]# /apps/spark-2.2.0-bin-hadoop2.7/bin/spark-shell --master spark://node00:7077 Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 18/11/05 11:48:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/11/05 11:49:04 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0 18/11/05 11:49:04 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException java.……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E8%BF%90%E8%A1%8C%E4%BB%BB%E5%8A%A1/" title="SparkShell运行任务">SparkShell运行任务</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            文章目录 * 1.Spark-Shell 交互式编程 * 1.1 启动命令 * 1.2 Spark-Shell中运行wordcount * 2\. spark-submit提交Job  开始本篇博客之前，请先准备好环境，参见 【上一篇 Spark集群部署】
1.Spark-Shell 交互式编程 1.1 启动命令 bin/spark-shell \ --master spark://l0:7077 \ --executor-memory 1g \ --total-executor-cores 2     参数 解释     –master 集群的master URL (如 spark://192.168.191.130:7077)   –executor-memory 1G 指定每个executor可用内存为1G   –total-executor-cores 2 指定每个executor使用的cup核数为2个   –jars 添加任务所需的其他依赖     如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。
 Spark Shell中已经默认将SparkContext类初始化为对象 sc 。用户代码如果需要用到，则直接应用sc即可。……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E8%BF%90%E8%A1%8C%E4%BB%BB%E5%8A%A1/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E8%BF%90%E8%A1%8Cspark%E4%BB%BB%E5%8A%A1%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/" title="sparkshell运行spark任务参数设置">sparkshell运行spark任务参数设置</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            之前初学spark用spark-shell执行小程序的时候, 每次执行action操作(比如count,collect或者println),都会报错:
 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
 同时如果去spark ui上(公司默认为ip:18080)会看到spark-shell为核数core为0:

原因是启动spark-shell的时候没有给他分配资源, 所以我们应该在启动spark-shell的时候这么写:
/home/mr/spark/bin/spark-shell --executor-memory 4G \ --total-executor-cores 10 \ --executor-cores 1  其中 :
--executor-memory 是指定每个executor(执行器)占用的内存
--total-executor-cores是所有executor总共使用的cpu核数
--executor-cores是每个executor使用的cpu核数
对于spark-shell还可以在yarn上执行:
--master yarn-client
这里必须是client,不同于spark-submit的yarn-cluster, 因为spark- shell作为一个与用户交互的命令行，必须将Driver运行在本地，而不是yarn上, 其他的参数与submit一样.
以上参数就限制了总cpu核数为10, executor数目为10
但是, 每次执行都要写这么多参数显然很麻烦, 我们也可以通过修改spark-shell的方法将以上参数改成默认, 方法如下:
spark-shell之前代码:
... ... function main() { ... else export SPARK_SUBMIT_OPTS &quot;$FWDIR&quot;/bin/spark-submit --class org.……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E8%BF%90%E8%A1%8Cspark%E4%BB%BB%E5%8A%A1%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/">阅读全文</a></p>
    </article>
    
    
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E8%AF%BB%E5%8F%96hbase%E6%95%B0%E6%8D%AE/" title="SparkShell读取HBase数据">SparkShell读取HBase数据</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            ……
        </div>
        <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E8%AF%BB%E5%8F%96hbase%E6%95%B0%E6%8D%AE/">阅读全文</a></p>
    </article>
    
    
    



<ol class="page-navigator">
    
    <li class="prev">
        <a href="https://zaina.newban.cn/posts/page/467/">上一页</a>
    </li>
    

    

    
        
        
    
    

    
        
        
        <li >
            <a href="https://zaina.newban.cn/posts/">1</a>
        </li>
        
    
        
        <li>
            <span>...</span>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/posts/page/466/">466</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/posts/page/467/">467</a>
        </li>
        
    
        
        
        <li  class="current">
            <a href="https://zaina.newban.cn/posts/page/468/">468</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/posts/page/469/">469</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/posts/page/470/">470</a>
        </li>
        
    
        
        <li>
            <span>...</span>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/posts/page/1960/">1960</a>
        </li>
        
    

    
    

    <li class="next">
        <a href="https://zaina.newban.cn/posts/page/469/">下一页</a>
    </li>
    
</ol>




</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>



<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>HDFS常用Shell命令和基础编程开发 | 开发者问答集锦</title>
    <meta property="og:title" content="HDFS常用Shell命令和基础编程开发 - 开发者问答集锦">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2020-09-02T00:00:00&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2020-09-02T00:00:00&#43;08:00'>
        
    <meta name="Keywords" content="">
    <meta name="description" content="HDFS常用Shell命令和基础编程开发">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/posts/hdfs%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4%E5%92%8C%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E5%BC%80%E5%8F%91/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">HDFS常用Shell命令和基础编程开发</h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            

<h1 id="hdfs常用shell命令">HDFS常用Shell命令</h1>

<p>Hadoop支持很多Shell命令，其中fs是HDFS最常用的命令，利用fs可以查看HDFS文件系统的目录结构、上传和下载数据、创建文件等。</p>

<p>HDFS有三种shell命令方式：</p>

<ol>
<li>hadoop fs :适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统。</li>
<li>Hadoop dfs:只能适用与HDFS文件系统。</li>
<li>hdfs dfs：跟hadoop dfs命令作用一样，也只能适用与HDfS文件系统。</li>
</ol>

<blockquote>
<p>我这里的的命令用的都是第三种，hdfs dfs。</p>
</blockquote>

<h2 id="对文件和文件夹的操作">对文件和文件夹的操作：</h2>

<pre><code>hdfs dfs -mkdir input
#在HDFS文件系统中创建一个'input'目录

hdfs dfs -ls input
#列出 input 目录下的内容

hdfs dfs -put /home/hadoop/myFlie.txt input
#将本地的文件myFile.txt上传到HDFS文件系统的input中。

hdfs dfs -get input/myFlie.txt /home/hadoop/下载
#从HDFS文件系统中下载文件到本地文件系统

hdfs dfs -cat input/myFlie.txt
#查看文件的全部内容

hdfs dfs -cp input/myFile.txt input
#在HDFS上复制文件

hdfs dfs -mv input/myFile.txt output
#在HDFS上移动文件

hdfs dfs -rm input/myFile.txt 
#从HDFS删除文件

hdfs dfs -du input/myFile.txt
#查看HDFS上某目录下所有文件大小，指定文件后显示具体文件大小

hdfs dfs -touchz input/file.txt
#创建一个0字节的空文件。

hdfs dfs -chmod 
#改名文件权限

hdfs dfs -chown 
#改变文件所有者
</code></pre>

<p><a href="https://img.it610.com/image/info8/2c7a56b8670446788d74393e57a01044.jpg"><img src="https://img.it610.com/image/info8/2c7a56b8670446788d74393e57a01044.jpg" alt="HDFS常用Shell命令和基础编程开发_第1张图片" /></a></p>

<p>图；操作示例</p>

<h2 id="hdfs-dfsadmin管理命令">HDFS dfsadmin管理命令：</h2>

<ol>
<li><code>hdfs dfsadmin -report</code></li>
</ol>

<p>查看文件系统的基本信息和统计信息。</p>

<ol>
<li><code>hdfs dfsadmin -safemode get/enter</code></li>
</ol>

<p>enter | leave | get |
wait：安全模式命令。安全模式是NameNode的一种状态，在这种状态下，NameNode不接受对名字空间的更改（只读）；不复制或删除块。NameNode在启动时自动进入安全模式，当配置块的最小百分数满足最小副本数的条件时，会自动离开安全模式。enter是进入，leave是离开。</p>

<ol>
<li><code>hdfs dfsadmin -refreshNodes</code></li>
</ol>

<p>重新读取hosts和exclude文件，使新的节点或需要退出集群的节点能够被NameNode重新识别。这个命令在新增节点或注销节点时用到。</p>

<ol>
<li><code>hdfs dfsadmin -finalizeUpgrade</code></li>
</ol>

<p>终结HDFS的升级操作。DataNode删除前一个版本的工作目录，之后NameNode也这样做。</p>

<ol>
<li><code>hdfs dfsadmin -fupgradeProgress</code></li>
</ol>

<p>status| details | force：请求当前系统的升级状态 | 升级状态的细节| 强制升级操作</p>

<ol>
<li><code>hdfs dfsadmin -metasave filename</code></li>
</ol>

<p>保存NameNode的主要数据结构到hadoop.log.dir属性指定的目录下的文件中。</p>

<p><a href="https://img.it610.com/image/info8/3ddaa8d89b0e45daa73c8b9a2e9c7d6f.jpg"><img src="https://img.it610.com/image/info8/3ddaa8d89b0e45daa73c8b9a2e9c7d6f.jpg" alt="HDFS常用Shell命令和基础编程开发_第2张图片" /></a></p>

<p>图：操作示例</p>

<h1 id="hdfs-api详解">HDFS API详解</h1>

<p>Hadoop中关于文件操作类基本上全部是在&rdquo;org.apache.hadoop.fs&rdquo;包中，这些API能够支持的操作包含：打开文件，读写文件，删除文件等。</p>

<p>Hadoop类库中最终面向用户提供的接口类是FileSystem，该类是个抽象类，只能通过来类的get方法得到具体类。get方法存在几个重载版本，常用的是这个：</p>

<pre><code>static FileSystem get(Configuration conf);
</code></pre>

<p>该类封装了几乎所有的文件操作，例如mkdir，delete等。综上基本上可以得出操作文件的程序库框架：</p>

<pre><code>operator()
{
    得到Configuration对象

    得到FileSystem对象

    进行文件操作
}
</code></pre>

<p>为了编写一个能够与HDFS交互的Java应用程序，一般需要向Java工程中添加以下JAR包：<br />
（1）”/usr/local/hadoop/share/hadoop/common”目录下的hadoop-common-2.7.1.jar和haoop-
nfs-2.7.1.jar；<br />
（2）/usr/local/hadoop/share/hadoop/common/lib”目录下的所有JAR包；<br />
（3）“/usr/local/hadoop/share/hadoop/hdfs”目录下的haoop-hdfs-2.7.1.jar和haoop-hdfs-
nfs-2.7.1.jar；<br />
（4）“/usr/local/hadoop/share/hadoop/hdfs/lib”目录下的所有JAR包。</p>

<h2 id="上传本地文件">上传本地文件：</h2>

<p>通过&rdquo;<code>FileSystem.copyFromLocalFile（Path src，Patch dst）</code>
&ldquo;可将本地文件上传到HDFS的制定位置上，其中src和dst均为文件的完整路径。具体代码如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;


public class CopyFile {


    public static void main(String[] args) throws Exception {

        Configuration conf = new Configuration();
        FileSystem hdfs = FileSystem.get(conf);

        //本地文件路径
        Path srcPath = new Path(&quot;/home/hadoop/myFile.txt&quot;);

        //HDFS路径
        Path dstPath = new Path(&quot;/input/&quot;);

        //进行文件上传
        hdfs.copyFromLocalFile(srcPath, dstPath);

        //打印hdfs的文件默认路径
        System.out.println(&quot;复制文件到： &quot; + conf.get(&quot;fs.default.name&quot;));

        FileStatus[] files= hdfs.listStatus(dstPath);

        //打印文件被复制到的路径
        for(FileStatus file:files)
            System.out.println(file.getPath());
    }
}
</code></pre>

<p>程序运行结果：</p>

<pre><code>复制文件到： file:///
file:/input
</code></pre>

<p>如果遇到因为文件权限不够，程序运行失败，解决方法如下：</p>

<blockquote>
<p>可能出现问题的原因有三种：</p>

<ol>
<li>hdfs 中的文件或文件夹 没有读取权限；</li>
<li>hdfs 的配置中未允许拷出文件；</li>
<li>linux 文件夹没有写入权限；</li>
</ol>

<p>针对上述三个原因，解决方法如下：</p>

<ol>
<li>增加hdfs文件夹权限</li>
</ol>

<p><code>hdfs dfs -chmod 777 /</code></p>

<ol>
<li><p>修改hdfs配置文件：</p>

<blockquote>
<pre><code># 在 $HADOOP_HOME/etc/hadoop/目录中，找到hdfs-site.xml，添加或更改以下属性：
</code></pre>
</blockquote>

<p><property>
       <name>dfs.permissionsname&gt;
       <value>falsevalue&gt;
property&gt;</p>

<h1 id="将true该为false">将true该为false。</h1></li>

<li><p>增加Linux文件夹权限：</p></li>
</ol>

<p><code>sudo chmod 777 /</code></p>
</blockquote>

<h2 id="创建hdfs文件">创建HDFS文件：</h2>

<p>通过&rdquo;FileSystem.create（Path f）&rdquo;可在HDFS上创建文件，其中f为文件的完整路径。具体代码如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
public class CreatFile {

    public static void main(String[] args) throws Exception {

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf); 

        //要输入文件的字符串
        byte[] s = &quot;hello hdfs\n&quot;.getBytes();   
        Path dfsPath = new Path(&quot;/text.txt&quot;);   
        FSDataOutputStream outputStream = hdfs.create(dfsPath);
        //写入文件
        outputStream.write(s, 0, s.length);
    }
}
</code></pre>

<h2 id="写入文件和读取文件">写入文件和读取文件：</h2>

<pre><code>import java.io.BufferedReader;
import java.io.InputStreamReader;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class ReadFile {

    public static void main(String[] args) throws Exception{

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        //要写入文件的内容
        byte[] wString = &quot;hello word! \nHello Hadoop!\nHello HDFS!\n&quot;.getBytes();

        //要写入的文件名
        String filename = &quot;file&quot;;

        FSDataOutputStream os = hdfs.create(new Path(filename));
        //写入文件
        os.write(wString, 0, wString.length);
        os.close();


        FSDataInputStream is = hdfs.open(new Path(filename));
        BufferedReader br = new BufferedReader(new InputStreamReader(is));

        //读取文件
        String line;
        while((line = br.readLine()) != null)
            System.out.println(line);

        is.close();
        br.close();//
        hdfs.close();   
    }
}
</code></pre>

<p>程序运行结果：</p>

<pre><code>hello word! 
Hello Hadoop!
Hello HDFS!
</code></pre>

<h2 id="创建hdfs目录">创建HDFS目录：</h2>

<p>通过&rdquo;FileSystem.mkdirs（Path f）&rdquo;可在HDFS上创建文件夹，其中f为文件夹的完整路径。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;


public class CreatDir {

    /**
     * @param args
     * @throws Exception 
     */
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        Path dfs = new Path(&quot;/TextDir&quot;);
        hdfs.mkdirs(dfs);

        System.out.println(hdfs.exists(dfs));
    }
}
</code></pre>

<h2 id="重命名hdfs文件">重命名HDFS文件：</h2>

<p>通过&rdquo;FileSystem.rename（Path src，Path
dst）&rdquo;可为指定的HDFS文件重命名，其中src和dst均为文件的完整路径。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;


public class Rename {

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        Path oldname = new Path(&quot;/text.txt&quot;);
        Path newname = new Path(&quot;/newtext.txt&quot;);

        hdfs.rename(oldname, newname);
        System.out.println(hdfs.exists(newname));
    }
}
</code></pre>

<h2 id="删除hdfs上的文件">删除HDFS上的文件：</h2>

<p>通过&rdquo;FileSystem.delete（Path f，Boolean
recursive）&rdquo;可删除指定的HDFS文件，其中f为需要删除文件的完整路径，recuresive用来确定是否进行递归删除。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class DeleteFile {

    public static void main(String[] args) throws Exception {       
        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);      
        FileSystem hdfs = FileSystem.get(conf);

        Path deletePath = new Path(&quot;/text4.txt&quot;);

        hdfs.delete(deletePath, true);
    }
}
</code></pre>

<blockquote>
<p>删除目录和删除文件代码一样，换成路径就行，如果目录下有文件，递归删除。</p>
</blockquote>

<h2 id="查看某个hdfs文件是否存在">查看某个HDFS文件是否存在：</h2>

<p>通过&rdquo;FileSystem.exists（Path f）&rdquo;可查看指定HDFS文件是否存在，其中f为文件的完整路径。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;


public class CheckFile {

    public static void main(String[] args) throws Exception{

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        Path findpPath = new Path(&quot;/input/myFile.txt&quot;);

        System.out.println(&quot;文件是否存在：&quot; + hdfs.exists(findpPath));
    }
}
</code></pre>

<p><a href="https://img.it610.com/image/info8/74f7f49c4f9e4c44a0aaab2dfd5c43ad.jpg"><img src="https://img.it610.com/image/info8/74f7f49c4f9e4c44a0aaab2dfd5c43ad.jpg" alt="HDFS常用Shell命令和基础编程开发_第3张图片" /></a></p>

<p>图：HDFS文件系统结构图</p>

<p>程序运行结果：</p>

<pre><code>文件是否存在：true
</code></pre>

<h2 id="查看hdfs文件的信息状态">查看HDFS文件的信息状态：</h2>

<p>通过&rdquo;FileSystem.getModificationTime()&ldquo;可查看指定HDFS文件的修改时间。具体实现如下：</p>

<pre><code>import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;

import javax.ws.rs.core.NewCookie;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;


public class GetTime {

    public static void main(String[] args) throws Exception{

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        Path findpPath = new Path(&quot;/input/myFile.txt&quot;);

        FileStatus fileStatus = hdfs.getFileStatus(findpPath);
        long accessTime = fileStatus.getAccessTime();
        long modeTime = fileStatus.getModificationTime();
        long size = fileStatus.getBlockSize();
        long len = fileStatus.getLen();
        String owner = fileStatus.getOwner();
        Path path = fileStatus.getPath();
        String group = fileStatus.getGroup();

        //将时间戳转换为格式化日期
        SimpleDateFormat sdf = new SimpleDateFormat();
        String time1 = sdf.format(new Date(accessTime));
        String time2 = sdf.format(new Date(modeTime));


        //获取文件的权限信息
        System.out.println(&quot;文件的权限：&quot; + fileStatus.getPermission());  
        System.out.println(&quot;文件创建时间：&quot; + time1);
        System.out.println(&quot;文件修改时间：&quot; + time2);
        System.out.println(&quot;HDFS文件块大小：&quot; + size);
        System.out.println(&quot;文件大小：&quot; + len);
        System.out.println(&quot;文件所有者：&quot; + owner);
        System.out.println(&quot;文件所在路径：&quot; + path);
        System.out.println(&quot;文件所属组：&quot; + group);

    }
}
</code></pre>

<p>程序运行结果：</p>

<pre><code>文件的权限：rw-r--r--
文件创建时间：18-6-9 下午1:59
文件修改时间：18-6-9 下午1:59
HDFS文件块大小：134217728
文件大小：37
文件所有者：hadoop
文件所在路径：hdfs://localhost:9000/input/myFile.txt
文件所属组：supergroup
</code></pre>

<h2 id="读取hdfs某个目录下的所有文件">读取HDFS某个目录下的所有文件：</h2>

<p>通过&rdquo;FileStatus.getPath（）&rdquo;可查看指定HDFS中某个目录下所有文件。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;

public class ReadDirFile {

    public static void main(String[] args) throws Exception {

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        Path dirPath = new Path(&quot;/input&quot;);

        FileStatus[] stats = hdfs.listStatus(dirPath);

        for(int i = 0; i &lt; stats.length; i++)
            System.out.println(stats[i].getPath().toString());
        hdfs.close();
    }
}
</code></pre>

<p><a href="https://img.it610.com/image/info8/df93ab0bdc304853bfc14eb1339ae940.jpg"><img src="https://img.it610.com/image/info8/df93ab0bdc304853bfc14eb1339ae940.jpg" alt="HDFS常用Shell命令和基础编程开发_第4张图片" /></a></p>

<p>图：input目录下有两文件</p>

<p>程序运行结果如下：</p>

<pre><code>hdfs://localhost:9000/input/myFile.txt
hdfs://localhost:9000/input/text2.txt
</code></pre>

<h2 id="查找某个文件在hdfs集群的位置">查找某个文件在HDFS集群的位置：</h2>

<p>通过&rdquo;FileSystem.getFileBlockLocation（FileStatus file，long start，long
len）&rdquo;可查找指定文件在HDFS集群上的位置，其中file为文件的完整路径，start和len来标识查找文件的路径。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.BlockLocation;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.protocol.BlockLocalPathInfo;


public class FileLoc {

    public static void main(String[] args) throws Exception {


        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);
        Path fPath = new Path(&quot;/input/myFile.txt&quot;);

        FileStatus status = hdfs.getFileStatus(fPath);

        BlockLocation[] blInfo = hdfs.getFileBlockLocations(status, 0, status.getLen());

        for(int i = 0; i &lt; blInfo.length; i++){
            String [] hosts = blInfo[i].getHosts();
            System.out.println(&quot;block: &quot; + i + &quot; Location: &quot; + hosts[0]);
        }
    }
}
</code></pre>

<p>程序运行结果：</p>

<pre><code>block: 0 Location: ubuntu
</code></pre>

<h2 id="获取hdfs集群上所有节点名称信息">获取HDFS集群上所有节点名称信息：</h2>

<p>通过&rdquo;DatanodeInfo.getHostName（）&rdquo;可获取HDFS集群上的所有节点名称。具体实现如下：</p>

<pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.hdfs.DistributedFileSystem;
import org.apache.hadoop.hdfs.protocol.DatanodeInfo;


public class GetInfo {

    public static void main(String[] args) throws Exception{

        Configuration conf = new Configuration();
        conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://localhost:9000&quot;);
        FileSystem hdfs = FileSystem.get(conf);

        DistributedFileSystem dFileSystem = (DistributedFileSystem) hdfs;
        DatanodeInfo[] dInfos = dFileSystem.getDataNodeStats();

        for(int i = 0; i &lt; dInfos.length; i++)
            System.out.println(&quot;DataNode_&quot; + i + &quot;_Name:&quot; + dInfos[i].getHostName());
    }
}
</code></pre>

<p>程序运行结果：</p>

<pre><code>DataNode_0_Name:ubuntu
</code></pre>

<hr />

<blockquote>
<p>以上内容为听华为大数据培训课程和大学MOOC上厦门大学 林子雨的《大数据技术原理与应用》课程而整理的笔记。</p>

<p><strong>大数据技术原理与应用</strong> ： <a href="https://www.icourse163.org/course/XMU-1002335004">https://www.icourse163.org/course/XMU-1002335004</a></p>
</blockquote>

<hr />

        </div>

        


        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a></li>
        
        <li><a href="/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/">009Shell脚本下条件测试eqne</a></li>
        
        <li><a href="/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/">00Pythonmanagepyshell和Python的分析</a></li>
        
        <li><a href="/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a></li>
        
        <li><a href="/posts/018dockerfileshell/">018DockerfileSHELL</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='https://zaina.newban.cn/tags/shell'>shell</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>3HDFS应用Shell | 开发者问答集锦</title>
    <meta property="og:title" content="3HDFS应用Shell - 开发者问答集锦">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2020-09-02T00:00:00&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2020-09-02T00:00:00&#43;08:00'>
        
    <meta name="Keywords" content="">
    <meta name="description" content="3HDFS应用Shell">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/posts/3hdfs%E5%BA%94%E7%94%A8shell/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">3HDFS应用Shell</h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            

<h2 id="一-概述">一. 概述</h2>

<ul>
<li>所有HDFS命令都由hadoop安装目录下的bin/hdfs脚本调用。运行不带任何参数的hdfs脚本会打印所有命令的描述。</li>
<li>用法：hdfs [SHELL_OPTIONS]命令 [GENERIC_OPTIONS] [COMMAND_OPTIONS]</li>
<li>HDFS命令主要分为两类：用户命令和管理命令。</li>
</ul>

<h2 id="二-用户命令">二. 用户命令</h2>

<p>**1. 用法：hadoop fs -appendToFile &hellip; **</p>

<p>将单个src或多个srcs从本地文件系统附加到目标文件系统。还从stdin读取输入并附加到目标文件系统。</p>

<ul>
<li>hadoop fs -appendToFile localfile /user/hadoop/hadoopfile</li>
<li>hadoop fs -appendToFile localfile1 localfile2 /user/hadoop/hadoopfile</li>
<li>hadoop fs -appendToFile localfile hdfs://nn.example.com/hadoop/hadoopfile</li>
<li>hadoop fs -appendToFile - hdfs://nn.example.com/hadoop/hadoopfile //从stdin读取输入</li>
</ul>

<p><strong>2. 用法：hadoop fs -cat URI [URI &hellip;]</strong></p>

<p>将源路径复制到stdout。</p>

<p>例：</p>

<ul>
<li>hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2</li>
</ul>

<p><strong>3. 用法：hadoop fs -checksum URI</strong></p>

<p>返回文件的校验和信息。</p>

<p>例：</p>

<ul>
<li>hadoop fs -checksum hdfs://nn1.example.com/file1</li>
</ul>

<p><strong>4. 用法：hadoop fs -chgrp [-R] GROUP URI [URI &hellip;]</strong></p>

<p>更改文件的组关联。用户必须是文件的所有者，否则必须是超级用户。</p>

<p>选项</p>

<ul>
<li>-R选项将通过目录结构递归地进行更改。</li>
</ul>

<p><strong>5. 用法：hadoop fs -chmod [-R] URI [URI &hellip;]</strong></p>

<p>更改文件的权限。使用-R，通过目录结构递归更改。用户必须是文件的所有者，否则必须是超级用户。</p>

<p>选项</p>

<ul>
<li>-R选项将通过目录结构递归地进行更改。</li>
</ul>

<p><strong>6. 用法：hadoop fs -chown [-R] [OWNER] [：[GROUP]] URI [URI]</strong></p>

<p>更改文件的所有者。用户必须是超级用户。</p>

<p>选项</p>

<ul>
<li>-R选项将通过目录结构递归地进行更改。</li>
</ul>

<p><strong>7. 用法：hadoop fs -chown [-R] [OWNER] [：[GROUP]] URI [URI]</strong></p>

<p>更改文件的所有者。用户必须是超级用户。</p>

<p>选项</p>

<ul>
<li>-R选项将通过目录结构递归地进行更改。</li>
</ul>

<p><strong>8. 用法：hadoop fs -copyFromLocal URI</strong></p>

<p>与put命令类似，但源仅限于本地文件引用。</p>

<p>选项：</p>

<ul>
<li>如果目标已存在，则-f选项将覆盖目标。</li>
</ul>

<p><strong>9. 用法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI</strong></p>

<p>与get命令类似，但目标仅限于本地文件引用。</p>

<p><strong>10. 用法：hadoop fs -cp [-f] [-p | -p [topax]] URI [URI &hellip;]</strong></p>

<p>将文件从源复制到目标。此命令也允许多个源，在这种情况下，目标必须是目录。</p>

<p>选项：</p>

<ul>
<li>如果目标已存在，则-f选项将覆盖目标。</li>
</ul>

<p>例：</p>

<ul>
<li>hadoop fs -cp /user/hadoop/file1/user/hadoop/file2</li>
</ul>

<p><strong>11. 用法：hadoop fs -df [-h] URI [URI &hellip;]</strong></p>

<p>显示可用空间。</p>

<p>选项：</p>

<ul>
<li>-h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864）</li>
</ul>

<p>例：</p>

<ul>
<li>hadoop dfs -df /user/ hadoop/dir1</li>
</ul>

<p><strong>12. 用法：hadoop fs -du [-s] [-h] URI [URI &hellip;]</strong></p>

<p>显示给定目录中包含的文件和目录的大小或文件的长度。</p>

<p>选项：</p>

<ul>
<li>-s选项将导致显示文件长度的汇总摘要，而不是单个文件。</li>
<li>-h选项将以“人类可读”的方式格式化文件大小（例如64.0m而不是67108864）</li>
</ul>

<p>例：</p>

<ul>
<li>hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1</li>
</ul>

<p><strong>13. 用法：hadoop fs -get</strong></p>

<p>将文件复制到本地文件系统。</p>

<p>例：</p>

<ul>
<li>hadoop fs -get /user/hadoop/file localfile</li>
<li>hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile</li>
</ul>

<p><strong>14. 用法：hadoop fs -ls [-d] [-h] [-R] [-t] [-S] [-r] [-u]</strong></p>

<p>选项：</p>

<ul>
<li>-d：目录列为纯文件。</li>
<li>-h：以人类可读的方式格式化文件大小（例如64.0m而不是67108864）。</li>
<li>-R：递归列出遇到的子目录。</li>
<li>-t：按修改时间排序输出（最近的第一个）。</li>
<li>-S：按文件大小排序输出。</li>
<li>-r：反转排序顺序。</li>
<li>-u：使用访问时间而不是修改时间进行显示和排序。</li>
</ul>

<p><strong>15. 用法：hadoop fs -lsr</strong></p>

<p>ls的递归版本。</p>

<p>注意：不推荐使用此命令。而是使用hadoop fs -ls -R</p>

<p><strong>16. 用法：hadoop fs -mkdir [-p]</strong></p>

<p>将路径uri作为参数并创建目录。</p>

<p>选项：</p>

<ul>
<li>-p选项行为与Unix mkdir -p非常相似，沿路径创建父目录。</li>
</ul>

<p>**17. 用法：hadoop fs -moveFromLocal **</p>

<p>与put命令类似，只是在复制后删除了源localsrc。</p>

<p><strong>18. 用法：hadoop fs -mv URI [URI &hellip;]</strong></p>

<p>将文件从源移动到目标。此命令也允许多个源，在这种情况下，目标需要是目录。不允许跨文件系统移动文件。</p>

<p>例：</p>

<ul>
<li>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</li>
</ul>

<p>**19. 用法：hadoop fs -put &hellip; **</p>

<p>将单个src或多个srcs从本地文件系统复制到目标文件系统。还从stdin读取输入并写入目标文件系统。</p>

<ul>
<li>hadoop fs -put localfile /user/hadoop/hadoopfile</li>
<li>hadoop fs -put localfile1 localfile2/user/hadoop/hadoopdir</li>
<li>hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile</li>
<li>hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile从stdin读取输入。</li>
</ul>

<p><strong>20. 用法：hadoop fs -rm [-f] [-r | -R] [-skipTrash] URI [URI &hellip;]</strong></p>

<p>删除指定为args的文件。</p>

<p>选项：</p>

<ul>
<li>如果文件不存在，-f选项将不显示诊断消息或修改退出状态以反映错误。</li>
<li>-R选项以递归方式删除目录及其下的任何内容。</li>
<li>-r选项等效于-R。</li>
</ul>

<p>例：</p>

<ul>
<li>hadoop fs -rm hdfs：//nn.example.com/file / user / hadoop / emptydir</li>
</ul>

<p><strong>21. 用法：hadoop fs -rmdir [&ndash;ignore-fail-on-non-empty] URI [URI &hellip;]</strong></p>

<p>删除目录。</p>

<p>选项：</p>

<ul>
<li>--ignore-fail-on-non-empty：使用通配符时，如果目录仍包含文件，请不要失败。</li>
</ul>

<p>例：</p>

<ul>
<li>hadoop fs -rmdir /user/hadoop/emptydir</li>
</ul>

<p><strong>22. 用法：hadoop fs -touchz URI [URI &hellip;]</strong></p>

<p>创建一个零长度的文件。</p>

<p><strong>23. 用法：hadoop fs -usage 命令</strong></p>

<p>返回单个命令的帮助。</p>

<p><strong>24. 用法：hadoop jar [mainClass] args &hellip;</strong></p>

<p>运行一个jar文件。</p>

<p>使用yarn jar来代替启动YARN应用程序。</p>

<p><strong>25. 用法：hdfs classpath</strong></p>

<p>打印获取Hadoop jar和所需库所需的类路径</p>

<p><strong>26. 用法：</strong></p>

<p>hdfs getconf -namenodes</p>

<p>hdfs getconf -secondaryNameNodes</p>

<p>hdfs getconf -backupNodes</p>

<p>hdfs getconf -includeFile</p>

<p>hdfs getconf -excludeFile</p>

<p>hdfs getconf -nnRpcAddresses</p>

<p>hdfs getconf -confKey [key]</p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-namenodes</td>
<td>获取集群中的名称节点列表。</td>
</tr>

<tr>
<td>-secondaryNameNodes</td>
<td>获取群集中的辅助名称节点列表。</td>
</tr>

<tr>
<td>-backupNodes</td>
<td>获取群集中的备份节点列表。</td>
</tr>

<tr>
<td>-includeFile</td>
<td>获取包含文件路径，该路径定义可以加入群集的数据节点。</td>
</tr>

<tr>
<td>-excludeFile</td>
<td>获取排除文件路径，该路径定义需要停用的数据节点。</td>
</tr>

<tr>
<td>-nnRpcAddresses</td>
<td>获取namenode rpc地址</td>
</tr>

<tr>
<td>-confKey [key]</td>
<td>从配置中获取特定密钥</td>
</tr>
</tbody>
</table>

<p><strong>27. 用法：hdfs groups [username &hellip;]</strong></p>

<p>返回给定一个或多个用户名的组信息。</p>

<p><strong>28. 用法：hdfs version</strong></p>

<p>打印版本。</p>

<h2 id="三-管理命令">三. 管理命令</h2>

<p><strong>1. 用法：</strong></p>

<p>hdfs balancer [-threshold ]</p>

<p>[-policy ]</p>

<p>[-exclude [-f  | ]]</p>

<p>[-include [-f  | ]]</p>

<p>[-idleiterations ]</p>

<p>-policy  | datanode（默认值）：如果每个datanode均衡，则群集是平衡的。<br />
blockpool：如果每个datanode中的每个块池都是平衡的，则群集是平衡的。<br />
&mdash;|&mdash;<br />
-threshold  | 磁盘容量的百分比。这会覆盖默认阈值。<br />
-exclude -f  |  | 排除指定的数据节点不被平衡器平衡。<br />
-include -f  |  | 仅包括由平衡器平衡的指定数据节点。<br />
-idleiterations  | 退出前的最大空闲迭代次数。这会覆盖默认的空闲状态（5）。</p>

<p>运行集群平衡实用程序。管理员只需按Ctrl-C即可停止重新平衡过程。</p>

<p><strong>2. 用法：</strong></p>

<p>hdfs dfsadmin [-report [-live] [-dead] [-decommissioning]]</p>

<p>[-safemode enter | leave| get| wait]</p>

<p>[-saveNamespace]</p>

<p>[-rollEdits]</p>

<p>[-restoreFailedStorage true | false | check]</p>

<p>[-refreshNodes] [-setQuota  &hellip; ]</p>

<p>[-clrQuota  &hellip; ]</p>

<p>[-setSpaceQuota  &hellip; ]</p>

<p>[-clspSpaceQuota  &hellip; ]</p>

<p>[-setStoragePolicy  ]</p>

<p>[-getStoragePolicy</p>

<p>[-metasave filename]</p>

<p>[-refreshServiceAcl]</p>

<p>[-refreshUserToGroupsMappings]</p>

<p>[-refreshSuperUserGroupsConfiguration]</p>

<p>[-refreshCallQueue]</p>

<p>[-refresh  [arg1..argn]]</p>

<p>[-reconfig  &lt; host：ipc_port&gt; ]</p>

<p>[-printTopology]</p>

<p>[-refreshNamenodes datanodehost:port]</p>

<p>[-deleteBlockPool datanode-host:port blockpoolId [force]]</p>

<p>[-setBalancerBandwidth ]</p>

<p>[-shutdownDatanode  [upgrade]]</p>

<p>[-getDatanodeInfo ]</p>

<p>[-triggerBlockReport [-incremental] ]</p>

<p>[-help [cmd]]</p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-report [-live] [-dead] [-decommissioning]</td>
<td>报告基本文件系统信息和统计信息。可选标志可用于过滤显示的DataNode列表。</td>
</tr>

<tr>
<td>-safemode enter</td>
<td>leave</td>
</tr>
</tbody>
</table>

<p>在Namenode启动时自动进入安全模式，并在配置的最小块百分比满足最小复制条件时自动离开安全模式。也可以手动输入安全模式，但也只能手动关闭。<br />
-saveNamespace | 将当前名称空间保存到存储目录并重置编辑日志。需要安全模式。<br />
-rollEdits | 在活动NameNode上滚动编辑日志。<br />
-restoreFailedStorage true | false | check | 此选项将打开/关闭自动尝试以还原失败的存储副本。如果故障存储再次可用，系统将尝试在检查点期间恢复编辑和/或fsimage。&rsquo;check&rsquo;选项将返回当前设置。<br />
-refreshNodes | 重新读取主机并排除文件以更新允许连接到Namenode的数据节点集以及应该停用或重新调试的数据节点集。<br />
-setQuota  &hellip;  | 有关详细信息，请参阅HDFS配额指南。<br />
-clrQuota  &hellip;  | 有关详细信息，请参阅HDFS配额指南。<br />
-setSpaceQuota  &hellip;  | 有关详细信息，请参阅HDFS配额指南。<br />
-clrSpaceQuota  &hellip;  | 有关详细信息，请参阅HDFS配额指南。<br />
-setStoragePolicy  | 将存储策略设置为文件或目录。<br />
-getStoragePolicy  | 获取文件或目录的存储策略。<br />
-finalizeUpgrade | 完成HDFS的升级。Datanodes删除其先前版本的工作目录，然后Namenode执行相同操作。这样就完成了升级过程。<br />
-rollingUpgrade [ |  | ] | 有关详细信息，请参见滚动升级文档<br />
-metasave 文件名 | 保存的Namenode的主要数据结构，文件名由hadoop.log.dir属性指定的目录。如果存在，则覆盖filename。filename将包含以下每一行的一行<br />
1.使用Namenode心跳的数据节点<br />
2.等待复制的<br />
块3.当前正在复制的<br />
块4.等待删除的块<br />
-refreshServiceAcl | 重新加载服务级别授权策略文件。<br />
-refreshUserToGroupsMappings | 刷新用户到组的映射。<br />
-refreshSuperUserGroupsConfiguration | 刷新超级用户代理组映射<br />
-refreshCallQueue | 从config重新加载呼叫队列。<br />
-refresh  [arg1..argn] | 触发上指定的资源的运行时刷新。之后的所有其他args被发送到主机。<br />
-reconfig  | 开始重新配置或获取正在进行的重新配置的状态。第二个参数指定节点类型。目前，仅支持重新加载DataNode的配置。<br />
-printTopology | 打印Namenode报告的机架树及其节点<br />
-refreshNamenodes datanodehost：port | 对于给定的datanode，重新加载配置文件，停止服务已删除的块池并开始提供新的块池。<br />
-deleteBlockPool datanode-host：port blockpoolId [force] | 如果传递force，则删除给定datanode上给定blockpool id的块池目录及其内容，否则仅当目录为空时才删除该目录。如果datanode仍在为块池提供服务，该命令将失败。请参阅refreshNamenodes以关闭datanode上的块池服务。<br />
-setBalancerBandwidth  | 在HDFS块平衡期间更改每个datanode使用的网络带宽。是每个datanode将使用的每秒最大字节数。此值将覆盖dfs.balance.bandwidthPerSec参数。注意：新值在DataNode上不是持久的。<br />
-fetchImage  | 从NameNode下载最新的fsimage并将其保存在指定的本地目录中。<br />
-shutdownDatanode  [upgrade] | 提交给定datanode的关闭请求。<br />
-getDatanodeInfo  | 获取有关给定datanode的信息。<br />
-triggerBlockReport [-incremental]  | 触发给定datanode的块报告。如果指定&rsquo;incremental&rsquo;，则是一个完整的块报告。<br />
-help [cmd] | 显示给定命令或所有命令的帮助（如果未指定）。</p>

<p><strong>3. 用法：</strong></p>

<p>hdfs haadmin -checkHealth</p>

<p>hdfs haadmin -failover [&ndash;forcefence] [&ndash;forceactive]</p>

<p>hdfs haadmin -getServiceState</p>

<p>hdfs haadmin -help</p>

<p>hdfs haadmin -transitionToActive  [ - -forceactive]</p>

<p>hdfs haadmin -transitionToStandby</p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-checkHealth</td>
<td>检查给定NameNode的运行状况</td>
</tr>

<tr>
<td>-failover</td>
<td>在两个NameNode之间启动故障转移</td>
</tr>

<tr>
<td>-getServiceState</td>
<td>确定给定的NameNode是活动还是待机</td>
</tr>

<tr>
<td>-transitionToActive</td>
<td>将给定NameNode的状态转换为Active（警告：不执行防护）</td>
</tr>

<tr>
<td>-transitionToStandby</td>
<td>将给定NameNode的状态转换为Standby（警告：没有完成防护）</td>
</tr>
</tbody>
</table>

<p><strong>4. 用法：hdfs journalnode</strong></p>

<p>此comamnd启动一个日志节点，用于与QJM一起使用HDFS HA。</p>

<p><strong>5. 用法：hdfs mover [-p | -f ]</strong></p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-f</td>
<td>指定包含要迁移的HDFS文件/目录列表的本地文件。</td>
</tr>

<tr>
<td>-p</td>
<td>指定要迁移的HDFS文件/目录的空格分隔列表。</td>
</tr>
</tbody>
</table>

<p>运行数据迁移实用程序。</p>

<p>请注意，当省略-p和-f选项时，默认路径是根目录。</p>

<p><strong>6. hdfs namenode [-backup]</strong></p>

<p>[-checkpoint]</p>

<p>[-format [-clusterid cid] [-force] [-nonInteractive]]</p>

<p>[-upgrade [-clusterid cid] [-renameReserved ]]</p>

<p>[-upgradeOnly [-clusterid cid] [-renameReserved ]]</p>

<p>[-rollback] | [-rollingUpgrade ] | [-finalize]</p>

<p>[-importCheckpoint] | [-initializeSharedEdits] | [-bootstrapStandby]</p>

<p>[-recover [-force]] | [-metadataVersion]</p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-backup</td>
<td>启动备份节点。</td>
</tr>

<tr>
<td>-checkpoint</td>
<td>启动检查点节点。</td>
</tr>

<tr>
<td>-format [-clusterid cid] [-force] [-nonInteractive]</td>
<td>格式化指定的NameNode。它启动NameNode，对其进行格式化然后将其关闭。如果名称目录存在，则为-force选项格式。如果名称目录存在，则-nonInteractive选项将中止，除非指定了-force选项。</td>
</tr>

<tr>
<td>-upgrade [-clusterid cid] [ -renameReserved ]</td>
<td>在分发新的Hadoop版本后，应该使用升级选项启动Namenode。</td>
</tr>

<tr>
<td>-upgradeOnly [-clusterid cid] [ -renameReserved ]</td>
<td>升级指定的NameNode然后关闭它。</td>
</tr>

<tr>
<td>-rollback</td>
<td>将NameNode回滚到以前的版本。这应该在停止集群并分发旧的Hadoop版本后使用。</td>
</tr>

<tr>
<td>-rollingUpgrade</td>
<td>有关详细信息，请参见滚动升级文档</td>
</tr>

<tr>
<td>-finalize</td>
<td>Finalize将删除文件系统的先前状态。最近的升级将成为永久性。回滚选项将不再可用。完成后，它会关闭NameNode。</td>
</tr>

<tr>
<td>-importCheckpoint</td>
<td>从检查点目录加载图像并将其保存到当前目录中。从属性fs.checkpoint.dir读取检查点目录</td>
</tr>

<tr>
<td>-initializeSharedEdits</td>
<td>格式化新的共享编辑目录并复制足够的编辑日志段，以便备用NameNode可以启动。</td>
</tr>

<tr>
<td>-bootstrapStandby</td>
<td>允许通过从活动NameNode复制最新的命名空间快照来引导备用NameNode的存储目录。首次配置HA群集时使用此选项。</td>
</tr>

<tr>
<td>-recover [-force]</td>
<td>在损坏的文件系统上恢复丢失的元数据。有关详细信息，请参阅HDFS用户指南。</td>
</tr>

<tr>
<td>-metadataVersion</td>
<td>验证配置的目录是否存在，然后打印软件和映像的元数据版本。</td>
</tr>
</tbody>
</table>

<p><strong>7. 用法：hdfs storagepolicies</strong></p>

<p>列出所有存储策略。</p>

<p><strong>8. 用法：hdfs zkfc [-formatZK [-force] [-nonInteractive]]</strong></p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-formatZK</td>
<td>格式化Zookeeper实例</td>
</tr>

<tr>
<td>-H</td>
<td>显示帮助</td>
</tr>
</tbody>
</table>

<p>此comamnd启动Zookeeper故障转移控制器进程，以便与HDJ HA和QJM一起使用。</p>

<p><strong>9. 用法：hdfs debug verify [-meta ] [-block ]</strong></p>

<table>
<thead>
<tr>
<th>命令选项</th>
<th>描述</th>
</tr>
</thead>

<tbody>
<tr>
<td>-block 块的文件</td>
<td>可选参数，用于指定数据节点的本地文件系统上的块文件的绝对路径。</td>
</tr>

<tr>
<td>-meta 元数据文件</td>
<td>数据节点的本地文件系统上的元数据文件的绝对路径。</td>
</tr>
</tbody>
</table>

<p>验证HDFS元数据和块文件。如果指定了块文件，我们将验证元数据文件中的校验和是否与块文件匹配。</p>

        </div>

        


        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a></li>
        
        <li><a href="/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/">009Shell脚本下条件测试eqne</a></li>
        
        <li><a href="/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/">00Pythonmanagepyshell和Python的分析</a></li>
        
        <li><a href="/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a></li>
        
        <li><a href="/posts/018dockerfileshell/">018DockerfileSHELL</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='https://zaina.newban.cn/tags/shell'>shell</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
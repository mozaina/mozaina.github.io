<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>spark02sparkshell使用scala开发spark | 开发者问答集锦</title>
    <meta property="og:title" content="spark02sparkshell使用scala开发spark - 开发者问答集锦">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2020-09-02T00:00:00&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2020-09-02T00:00:00&#43;08:00'>
        
    <meta name="Keywords" content="">
    <meta name="description" content="spark02sparkshell使用scala开发spark">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/posts/spark02sparkshell%E4%BD%BF%E7%94%A8scala%E5%BC%80%E5%8F%91spark/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">spark02sparkshell使用scala开发spark</h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            

<h3 id="文章目录">文章目录</h3>

<ul>
<li>Spark角色介绍</li>
<li>spark任务提交以及spark-shell使用</li>
<li>运行spark-shell &ndash;master local[N] 读取hdfs上面的文件</li>
<li>使用scala开发spark程序代码本地运行</li>
</ul>

<h1 id="spark角色介绍">Spark角色介绍</h1>

<p>Spark是基于内存计算的大数据并行计算框架。因为其基于内存计算，比Hadoop中MapReduce计算框架具有更高的实时性，同时保证了高效容错性和可伸缩性。从2009年诞生于AMPLab到现在已经成为Apache顶级开源项目，并成功应用于商业集群中，学习Spark就需要了解其架构。<br />
Spark架构图如下：<br />
<a href="https://img.it610.com/image/info8/05116f3f6bf44fdca05a6cd79fc902d2.jpg"><img src="https://img.it610.com/image/info8/05116f3f6bf44fdca05a6cd79fc902d2.jpg" alt="spark02\(spark-
shell使用，scala开发spark\)_第1张图片" /></a><br />
Spark架构使用了分布式计算中master-slave模型，master是集群中含有master进程的节点，slave是集群中含有worker进程的节点。<br />
 Driver Program ：运⾏main函数并且新建SparkContext的程序。<br />
 Application：基于Spark的应用程序，包含了driver程序和集群上的executor。<br />
 Cluster Manager：指的是在集群上获取资源的外部服务。目前有三种类型<br />
（1）Standalone: spark原生的资源管理，由Master负责资源的分配<br />
（2）Apache Mesos:与hadoop MR兼容性良好的一种资源调度框架<br />
（3）Hadoop Yarn: 主要是指Yarn中的ResourceManager<br />
 Worker Node：
集群中任何可以运行Application代码的节点，在Standalone模式中指的是通过slaves文件配置的Worker节点，在Spark on
Yarn模式下就是NodeManager节点<br />
 Executor：是在一个worker
node上为某应⽤启动的⼀个进程，该进程负责运⾏任务，并且负责将数据存在内存或者磁盘上。每个应⽤都有各自独立的executor。<br />
 Task ：被送到某个executor上的工作单元。</p>

<h1 id="spark任务提交以及spark-shell使用">spark任务提交以及spark-shell使用</h1>

<p>spark任务提交说明<br />
一旦打包好,就可以使用bin/spark-submit脚本启动应用了.
这个脚本负责设置spark使用的classpath和依赖,支持不同类型的集群管理器和发布模式:</p>

<pre><code>bin/spark-submit \
  --class &lt;main-class&gt;
  --master &lt;master-url&gt; \
  --deploy-mode &lt;deploy-mode&gt; \
  --conf &lt;key&gt;=&lt;value&gt; \
  ... # other options
  &lt;application-jar&gt; \
  [application-arguments]
</code></pre>

<p>一些常用选项:<br />
–class: 你的应用的启动类 (如 org.apache.spark.examples.SparkPi)</p>

<ol>
<li>–master: 集群的master URL (如 spark://node01:7077)</li>
<li>–deploy-mode: 是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*</li>
<li>–conf: 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value”. 缺省的Spark配置</li>
<li>application-jar: 打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar.</li>

<li><p>application-arguments: 传给main()方法的参数<br />
Master URL 可以是以下格式：<br />
查看Spark-submit全部参数：<br />
<a href="https://img.it610.com/image/info8/649758b46dd643a5a67bef7a978d1ae5.jpg"><img src="https://img.it610.com/image/info8/649758b46dd643a5a67bef7a978d1ae5.jpg" alt="spark02\(spark-
shell使用，scala开发spark\)_第2张图片" /></a><br />
更多参数提交说明：</p>

<p>–master MASTER_URL
可以是spark://host:port, mesos://host:port, yarn, yarn-cluster,yarn-client, local
–deploy-mode DEPLOY_MODE
Driver程序运行的地方，client或者cluster
–class CLASS_NAME
主类名称，含包名
–name NAME
Application名称
–jars JARS
Driver依赖的第三方jar包
–py-files PY_FILES
用逗号隔开的放置在Python应用程序PYTHONPATH上的.zip, .egg, .py文件列表
–files FILES
用逗号隔开的要放置在每个executor工作目录的文件列表
–properties-file FILE
设置应用程序属性的文件路径，默认是conf/spark-defaults.conf
–driver-memory MEM
Driver程序使用内存大小
–driver-java-options
–driver-library-path
Driver程序的库路径
–driver-class-path
Driver程序的类路径
–executor-memory MEM
executor内存大小，默认1G
–driver-cores NUM
Driver程序的使用CPU个数，仅限于Spark Alone模式
–supervise
失败后是否重启Driver，仅限于Spark Alone模式
–total-executor-cores NUM
executor使用的总核数，仅限于Spark Alone、Spark on Mesos模式
–executor-cores NUM
每个executor使用的内核数，默认为1，仅限于Spark on Yarn模式
–queue QUEUE_NAME
提交应用程序给哪个YARN的队列，默认是default队列，仅限于Spark on Yarn模式
–num-executors NUM
启动的executor数量，默认是2个，仅限于Spark on Yarn模式
–archives ARCHIVES
仅限于Spark on Yarn模式</p></li>
</ol>

<p>启动Spark Shell<br />
运行spark-shell &ndash;master local[N] 读取本地文件</p>

<p>单机模式：通过本地N个线程跑任务，只运行一个SparkSubmit进程。<br />
创建本地文件，使用spark程序实现单词计数统计<br />
第一步：准备本地文件<br />
node01服务器执行以下命令准备数据文件</p>

<pre><code>mkdir -p /export/servers/sparkdatas
cd /export/servers/sparkdatas/
vim wordcount.txt

hello me
hello you
hello her
</code></pre>

<p>第二步：通 &ndash;master启动本地模式<br />
node01执行以下命令进入spark-shell</p>

<pre><code>bin/spark-shell --master local[2]
</code></pre>

<p>第三步：开发scala单词统计代码<br />
使用这种方式</p>

<pre><code>sc.textFile(&quot;file:///export/servers/sparkdatas/wordcount.txt&quot;).flatMap(x =&gt; x.split(&quot; &quot;)).map(x =&gt; (x,1)).reduceByKey((x,y) =&gt; x + y).collect
</code></pre>

<p>或者使用以下这种方式，通过下划线来进行替代</p>

<pre><code>sc.textFile(&quot;file:///export/servers/sparkdatas/wordcount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).collect
</code></pre>

<p>代码说明：<br />
sc：Spark-Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可。<br />
textFile:读取数据文件<br />
flatMap:对文件中的每一行数据进行压平切分,这里按照空格分隔。<br />
map:对出现的每一个单词记为1（word，1）<br />
reduceByKey:对相同的单词出现的次数进行累加<br />
collect:触发任务执行，收集结果数据。</p>

<h1 id="运行spark-shell-master-local-n-读取hdfs上面的文件">运行spark-shell &ndash;master local[N] 读取hdfs上面的文件</h1>

<p>第一步：将我们的数据文件上传hdfs</p>

<pre><code>cd /export/servers/sparkdatas
hdfs dfs -mkdir -p /sparkwordcount
hdfs dfs -put wordcount.txt  /sparkwordcountss
</code></pre>

<p>第二步：开发spark的程序</p>

<pre><code>sc.textFile(&quot;hdfs://node01:8020/sparkwordcount/wordcount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).collect
</code></pre>

<p>运行spark-shell &ndash;master spark://node01:7077<br />
我们可以通过 –master spark://node01:7077来进行提交我们的spark程序到spark集群上面去运行<br />
退出之前运行的spark-shell，然后重新进入spark-shell并且指定我们的spark的主节点地址<br />
第一步：重新进入spark-shell<br />
退出之前的本地spark-shell模式:quit，然后执行以下命令重新进入spark-shell客户端</p>

<pre><code>bin/spark-shell --master spark://node01:7077,node02:7077 \
 --executor-memory 1g \
 --total-executor-cores 2       
</code></pre>

<p>第二步：开发spark程序<br />
开发我们的spark程序，读取hdfs上面的数据，进行wordcount单词统计</p>

<pre><code>sc.textFile(&quot;hdfs://node01:8020/sparkwordcount/wordcount.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_ + _).collect
</code></pre>

<p><a href="https://img.it610.com/image/info8/3a575890ad9f48abb43491c687113e42.jpg"><img src="https://img.it610.com/image/info8/3a575890ad9f48abb43491c687113e42.jpg" alt="spark02\(spark-
shell使用，scala开发spark\)_第3张图片" /></a></p>

<h1 id="使用scala开发spark程序代码本地运行">使用scala开发spark程序代码本地运行</h1>

<p>第一步：创建maven工程并导入jar包<br />
记得创建src/main/scala以及 src/test/scala文件夹</p>

<pre><code>&lt;properties&gt;
        &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;
        &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;
    &lt;/properties&gt;
    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
            &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
            &lt;version&gt;${scala.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
            &lt;version&gt;${spark.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
            &lt;version&gt;2.7.5&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
    &lt;build&gt;
        &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;
        &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.0&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
                    &lt;!--    &lt;verbal&gt;true&lt;/verbal&gt;--&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
                &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2.0&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;compile&lt;/goal&gt;
                            &lt;goal&gt;testCompile&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;args&gt;
                                &lt;arg&gt;-dependencyfile&lt;/arg&gt;
                                &lt;arg&gt;${project.build.directory}/.scala_dependencies&lt;/arg&gt;
                            &lt;/args&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.1.1&lt;/version&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;shade&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;filters&gt;
                                &lt;filter&gt;
                                    &lt;artifact&gt;*:*&lt;/artifact&gt;
                                    &lt;excludes&gt;
                                        &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;
                                        &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;
                                        &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;
                                    &lt;/excludes&gt;
                                &lt;/filter&gt;
                            &lt;/filters&gt;
                            &lt;transformers&gt;
                                &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;
                                    &lt;mainClass&gt;&lt;/mainClass&gt;
                                &lt;/transformer&gt;
                            &lt;/transformers&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
</code></pre>

<p>第二步：创建scala的object并开发scala代码</p>

<pre><code>object SparkFirst {

  def main(args: Array[String]): Unit = {
    //定义我们的配置文件对象
    val sparkConf: SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;sparkLocalCount&quot;)
    //获取sparkContext对象，用于操作我们的数据
    val sparkContext = new SparkContext(sparkConf)
    //设置日志级别，避免打印太多日志
    sparkContext.setLogLevel(&quot;WARN&quot;)
    //读取本地文件
    val file: RDD[String] = sparkContext.textFile(&quot;file:///F:\\scala与spark课件资料教案\\3、spark课程\\1、spark第一天\\wordcount\\wordcount.txt&quot;)
    //每一行文件进行拆分
    val flatMap: RDD[String] = file.flatMap( x =&gt; x.split(&quot; &quot;))
    //拆分出来的每个单词记做1
    val map: RDD[(String, Int)] = flatMap.map(x =&gt; (x,1))
    //按照我们的单词进行划分，相同的单词划分到一起去
    val key: RDD[(String, Int)] = map.reduceByKey((x,y) =&gt; x)
    //按照单词出现的次数进行排序,false表示降序，默认是升序
    val by: RDD[(String, Int)] = key.sortBy(x =&gt; x._2,false)
    //调用collect收集我们的结果，然后通过println打印出来
    val collect: Array[(String, Int)] = by.collect()
    println(collect.toBuffer)
    //将我们最终的结果保存到文件里面去
    key.saveAsTextFile(&quot;file:///F:\\scala与spark课件资料教案\\3、spark课程\\1、spark第一天\\wordcount\\output2\\result.txt&quot;)
    //关闭sparkContext
    sparkContext.stop()
  }

}
</code></pre>

<p>第三步：通过main方法本地运行<br />
直接右键通过main方法进行本地运行</p>

<p>第四步：更改代码打包提交到spark集群运行<br />
改造代码</p>

<pre><code>import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object WordCount {
  def main(args: Array[String]): Unit = {
    //设置spark的配置文件信息
  //  val sparkConf: SparkConf = new SparkConf().setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;)
    val sparkConf: SparkConf = new SparkConf().setAppName(&quot;WordCount&quot;)

    //构建sparkcontext上下文对象，它是程序的入口,所有计算的源头
    val sc: SparkContext = new SparkContext(sparkConf)
    //读取文件
    //val file: RDD[String] = sc.textFile(&quot;file:///F:\\scala与spark课件资料教案\\spark课程\\1、spark第一天\\wordcount\\wordcount.txt&quot;)
    val file: RDD[String] = sc.textFile(args(0))

    //对文件中每一行单词进行压平切分
    val words: RDD[String] = file.flatMap(_.split(&quot; &quot;))
    //对每一个单词计数为1 转化为(单词，1)
    val wordAndOne: RDD[(String, Int)] = words.map(x=&gt;(x,1))
    //相同的单词进行汇总 前一个下划线表示累加数据，后一个下划线表示新数据
    val result: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)
    //保存数据到HDFS
// result.saveAsTextFile(&quot;file:///F:\\scala与spark课件资料教案\\spark课程\\1、spark第一天\\wordcount\\wordcount&quot;)
    result.saveAsTextFile(args(1))
    sc.stop()
  }
}
</code></pre>

<p>第五步：打包上传到linux集群<br />
<a href="https://img.it610.com/image/info8/f42858b593ad4777bed8aa984973f0b6.jpg"><img src="https://img.it610.com/image/info8/f42858b593ad4777bed8aa984973f0b6.jpg" alt="spark02\(spark-
shell使用，scala开发spark\)_第4张图片" /></a><br />
第六步：运行spark的jar包程序<br />
通过spark-submit脚本来提交我们开发的jar包</p>

<pre><code>bin/spark-submit  \
--class cn.itcast.spark.wordcount.WordCount \
--master spark://node01:7077 \
--executor-memory 1g \
--total-executor-cores 2 \
/export/servers/original-day01-1.0-SNAPSHOT.jar \
hdfs://node01:8020/sparkwordcount/wordcount.txt  \
hdfs://node01:8020/sparkwordcount_out  
</code></pre>

        </div>

        


        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a></li>
        
        <li><a href="/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/">009Shell脚本下条件测试eqne</a></li>
        
        <li><a href="/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/">00Pythonmanagepyshell和Python的分析</a></li>
        
        <li><a href="/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a></li>
        
        <li><a href="/posts/018dockerfileshell/">018DockerfileSHELL</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='https://zaina.newban.cn/tags/shell'>shell</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
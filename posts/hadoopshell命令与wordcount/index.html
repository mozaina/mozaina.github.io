<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>HadoopShell命令与WordCount | 开发者问答集锦</title>
    <meta property="og:title" content="HadoopShell命令与WordCount - 开发者问答集锦">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2020-09-02T00:00:00&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2020-09-02T00:00:00&#43;08:00'>
        
    <meta name="Keywords" content="">
    <meta name="description" content="HadoopShell命令与WordCount">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/posts/hadoopshell%E5%91%BD%E4%BB%A4%E4%B8%8Ewordcount/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">HadoopShell命令与WordCount</h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            

<h3 id="前言">前言</h3>

<p>在前2章内, 我们分别介绍了Hadoop安装的3种形式(<code>Standalone mode</code>/ <code>Pseudo-Distributed
mode</code>/<code>Cluster mode</code>). 本章, 我们介绍如何使用HDFS命令进行一些基本的操作. 官方的操作文档可以查看Hadoop Shell命令.</p>

<hr />

<h3 id="正文">正文</h3>

<h5 id="前置条件">前置条件</h5>

<p>已经安装<code>Hadoop</code>集群, 并启动. 从页面可以看到, 我们<code>HDFS</code>系统的文件目录.<br />
<a href="https://img.it610.com/image/info8/a0c498df766b48c7ae0081b59a7bcb3e.jpg"><img src="https://img.it610.com/image/info8/a0c498df766b48c7ae0081b59a7bcb3e.jpg" alt="Hadoop Shell 命令 与
WordCount_第1张图片" /></a></p>

<h5 id="基本操作">基本操作</h5>

<p>对于文件系统用的最多的就是, <code>增删查改</code>与<code>权限系统</code>一直是我们操作文件系统的基本命令.它们的基本操作分别如下所示:</p>

<ul>
<li><p>查看文件目录 <code>ls</code></p>

<h1 id="本地仓库">本地仓库</h1>

<p>localhost:current Sean$ hadoop fs -ls /
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
19/03/30 16:15:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform&hellip; using builtin-java classes where applicable
Found 3 items
-rw-r&ndash;r&ndash;   1 Sean supergroup          2 2019-03-25 11:55 /1.log
drwx&mdash;&mdash;   - Sean supergroup          0 2019-03-25 12:11 /tmp
drwxr-xr-x   - Sean supergroup          0 2019-03-25 13:16 /user</p>

<h1 id="全路径">全路径</h1>

<p>localhost:current Sean$ hadoop fs -ls hdfs://localhost:9000/
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
19/03/30 16:16:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform&hellip; using builtin-java classes where applicable
Found 3 items
-rw-r&ndash;r&ndash;   1 Sean supergroup          2 2019-03-25 11:55 hdfs://localhost:<sup>9000</sup>&frasl;<sub>1</sub>.log
drwx&mdash;&mdash;   - Sean supergroup          0 2019-03-25 12:11 hdfs://localhost:9000/tmp
drwxr-xr-x   - Sean supergroup          0 2019-03-25 13:16 hdfs://localhost:9000/user</p></li>

<li><p>上传文件 <code>put</code></p>

<h1 id="上传文件">上传文件</h1>

<p>localhost:current Sean$ hadoop fs -put hello2019.sh /</p>

<h1 id="查询上传的文件">查询上传的文件</h1>

<p>localhost:current Sean$ hadoop fs -ls /
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
Found 4 items
-rw-r&ndash;r&ndash;   1 Sean supergroup          2 2019-03-25 11:55 /1.log
-rw-r&ndash;r&ndash;   1 Sean supergroup         10 2019-03-30 16:19 /hello2019.sh
drwx&mdash;&mdash;   - Sean supergroup          0 2019-03-25 12:11 /tmp
drwxr-xr-x   - Sean supergroup          0 2019-03-25 13:16 /user</p>

<h1 id="手动合并-文件是可以还原的">手动合并(文件是可以还原的.)</h1>

<p>cat blk_1073741983 &gt;&gt; tmp.file
cat blk_1073741984 &gt;&gt; tmp.file</p></li>
</ul>

<p>默认文件切分大小为128M, 大于的话会切分成2快.</p>

<ul>
<li><p>查看文件内容 <code>cat</code></p>

<h1 id="通过hadoop查看">通过hadoop查看</h1>

<p>localhost:current Sean$ hadoop fs -cat /hello2019.sh
hello2019</p>

<p>#通过本地linux查看
localhost:current Sean$ cat finalized/subdir0/subdir0/blk_1073741983
hello2019</p>

<p>localhost:current Sean$ pwd
/Users/Sean/Software/hadoop/current/tmp/dfs/data/current/BP-586017156-127.0.0.1-1553485799471/current</p></li>

<li><p>下载文件 <code>get</code></p>

<p>localhost:current Sean$ hadoop fs -get /hello2019.sh
localhost:current Sean$ ls
VERSION     dfsUsed     finalized   hello2019.sh    rbw
localhost:current Sean$ cat hello2019.sh
hello2019</p></li>

<li><p>创建文件目录 <code>mkdir</code></p>

<p>localhost:current Sean$ hadoop fs -mkdir -p /wordcount/input
localhost:current Sean$ hadoop fs -ls /wordcount
Found 1 items
drwxr-xr-x   - Sean supergroup          0 2019-03-30 16:40 /wordcount/input</p></li>
</ul>

<hr />

<h3 id="others">Others</h3>

<p>hdfs的命令操作, 可以通过<code>hadoop fs</code>直接显示所以命令.</p>

<pre><code>localhost:mapreduce Sean$ hadoop fs
Usage: hadoop fs [generic options]
    [-appendToFile  ... ]
    [-cat [-ignoreCrc]  ...]
    [-checksum  ...]
    [-chgrp [-R] GROUP PATH...]
    [-chmod [-R]  PATH...]
    [-chown [-R] [OWNER][:[GROUP]] PATH...]
    [-copyFromLocal [-f] [-p] [-l]  ... ]
    [-copyToLocal [-p] [-ignoreCrc] [-crc]  ... ]
    [-count [-q] [-h]  ...]
    [-cp [-f] [-p | -p[topax]]  ... ]
    [-createSnapshot  []]
    [-deleteSnapshot  ]
    [-df [-h] [ ...]]
    [-du [-s] [-h]  ...]
    [-expunge]
    [-find  ...  ...]
    [-get [-p] [-ignoreCrc] [-crc]  ... ]
    [-getfacl [-R] ]
    [-getfattr [-R] {-n name | -d} [-e en] ]
    [-getmerge [-nl]  ]
    [-help [cmd ...]]
    [-ls [-d] [-h] [-R] [ ...]]
    [-mkdir [-p]  ...]
    [-moveFromLocal  ... ]
    [-moveToLocal  ]
    [-mv  ... ]
    [-put [-f] [-p] [-l]  ... ]
    [-renameSnapshot   ]
    [-rm [-f] [-r|-R] [-skipTrash]  ...]
    [-rmdir [--ignore-fail-on-non-empty]  ...]
    [-setfacl [-R] [{-b|-k} {-m|-x } ]|[--set  ]]
    [-setfattr {-n name [-v value] | -x name} ]
    [-setrep [-R] [-w]   ...]
    [-stat [format]  ...]
    [-tail [-f] ]
    [-test -[defsz] ]
    [-text [-ignoreCrc]  ...]
    [-touchz  ...]
    [-truncate [-w]   ...]
    [-usage [cmd ...]]

Generic options supported are
-conf      specify an application configuration file
-D             use value for given property
-fs       specify a namenode
-jt     specify a ResourceManager
-files     specify comma separated files to be copied to the map reduce cluster
-libjars     specify comma separated jar files to include in the classpath.
-archives     specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]
</code></pre>

<ul>
<li><p><code>help</code><br />
输出命令参数手册.</p></li>

<li><p><code>mkdir</code><br />
创建目录. <code>hadoop fs -mkdir -p /abc/acc</code></p></li>

<li><p><code>moveFromLocal / moveToLocal</code><br />
从本地移动HDFS(本地原文件删除). <code>hadoop fs -moveFromLocal abc.txt /</code><br />
从HDFS移动本地(HDFS原文件删除). <code>hadoop fs -moveFromLocal abc.txt /</code></p></li>

<li><p><code>appendToFile</code><br />
追加到文件上面. <code>hadoop fs -appendToFile abc.txt /hello2019.txt</code></p>

<p>localhost:mapreduce Sean$ echo xxoo &gt;&gt; hello.txt
localhost:mapreduce Sean$ hadoop fs -appendToFile hello.txt /hello2019.sh
localhost:mapreduce Sean$ hadoop fs -cat /hello2019.sh
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
hello2019
xxoo</p></li>

<li><p><code>cat</code><br />
显示文件. <code>hadoop fs -cat /hello2019.sh</code>. 文件过多<code>hadoop fs -cat /hello2019.sh |
more</code>或者<code>hadoop fs -tail /hello2019.sh</code></p></li>

<li><p><code>tail</code><br />
显示文件末尾. <code>hadoop fs -tail /hello2019.sh</code></p></li>

<li><p><code>text</code><br />
已字符形式打印一个文件的内容.<code>hadoop fs -text /hello2019.sh</code>.</p></li>

<li><p><code>chgrp / chmod / chown</code><br />
<code>chgrp</code>更改文件组; <code>chmod</code>更改权限; <code>chown</code>更改用户和组.</p>

<p>hadoop fs -chmod 666 /hello2019.txt
hadoop fs -chown someuser:somegrp /hello2019.txt</p>

<p>localhost:mapreduce Sean$ hadoop fs -chmod 777 /hello2019.sh
localhost:mapreduce Sean$ hadoop fs -ls /
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
19/03/30 17:09:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform&hellip; using builtin-java classes where applicable
Found 5 items
-rw-r&ndash;r&ndash;   1 Sean supergroup          2 2019-03-25 11:55 /1.log
-rwxrwxrwx   1 Sean supergroup         15 2019-03-30 16:55 /hello2019.sh
drwx&mdash;&mdash;   - Sean supergroup          0 2019-03-25 12:11 /tmp
drwxr-xr-x   - Sean supergroup          0 2019-03-25 13:16 /user
drwxr-xr-x   - Sean supergroup          0 2019-03-30 16:43 /wordcount</p>

<h1 id="hadoop内没有用户的设计-所以没创建该用户也可以这样改造">hadoop内没有用户的设计.(所以没创建该用户也可以这样改造.)</h1>

<p>localhost:mapreduce Sean$ hadoop fs -chown hellokitty:hello  /hello2019.sh
localhost:mapreduce Sean$ hadoop fs -ls /
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
19/03/30 17:10:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform&hellip; using builtin-java classes where applicable
Found 5 items
-rw-r&ndash;r&ndash;   1 Sean       supergroup          2 2019-03-25 11:55 /1.log
-rwxrwxrwx   1 hellokitty hello              15 2019-03-30 16:55 /hello2019.sh
drwx&mdash;&mdash;   - Sean       supergroup          0 2019-03-25 12:11 /tmp
drwxr-xr-x   - Sean       supergroup          0 2019-03-25 13:16 /user
drwxr-xr-x   - Sean       supergroup          0 2019-03-30 16:43 /wordcount</p></li>

<li><p><code>copyFromlocal / copyToLocal</code><br />
从本地拷贝; 拷贝到本地</p></li>

<li><p><code>cp</code><br />
hdfs内部进行拷贝. <code>hadoop fs -cp /hello2019.sh /a/hello2019.sh</code></p></li>

<li><p><code>mv</code><br />
hdfs内部进行移动. <code>hadoop fs -mv /hello2019.sh /a/</code></p></li>

<li><p><code>get</code><br />
获取到本地. 类似<code>copyToLocal</code>. <code>hadoop fs -get /hello.sh</code></p></li>

<li><p><code>getmerge</code><br />
合并下载多个文件. <code>hadoop fs -getmerge /wordcount/output/* hellomerge.sh</code></p>

<p>localhost:mapreduce Sean$ hadoop fs -getmerge /wordcount/output/* hellomerge.sh
localhost:mapreduce Sean$ cat hellomerge.sh
2019    1
able    1
cat 2
hello   1
kitty   1
pitty   2</p></li>

<li><p><code>put</code><br />
下载到本地. 类似<code>copyFromLocal</code>. <code>hadoop fs -put hello2019.sh /</code></p></li>

<li><p><code>rm</code><br />
删除. <code>hadoop fs -rm -r /hello2019.sh</code></p>

<h1 id="r-recursive-递归的意思">-r recursive 递归的意思</h1>

<p>localhost:mapreduce Sean$ hadoop fs -rm -r /1.log
Deleted /1.log
localhost:mapreduce Sean$ hadoop fs -ls /
Found 4 items
-rwxrwxrwx   1 hellokitty hello              15 2019-03-30 16:55 /hello2019.sh
drwx&mdash;&mdash;   - Sean       supergroup          0 2019-03-25 12:11 /tmp
drwxr-xr-x   - Sean       supergroup          0 2019-03-25 13:16 /user
drwxr-xr-x   - Sean       supergroup          0 2019-03-30 16:43 /wordcount</p></li>

<li><p><code>rmdir</code><br />
删除空目录. <code>hadoop fs - rmdir /abbc</code></p></li>

<li><p><code>df</code><br />
统计文件系统的可用信息. <code>hadoop fs -df -h /</code></p>

<p>localhost:mapreduce Sean$ hadoop fs -df -h /
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
Filesystem                Size   Used  Available  Use%
hdfs://localhost:9000  465.7 G  5.9 M    169.1 G    0%</p></li>

<li><p><code>du</code><br />
统计文件夹目录的大小. <code>hadoop fs -du -s -h /abc/d</code></p>

<h1 id="s-汇总-h-带单位">-s 汇总 -h 带单位</h1>

<p>localhost:mapreduce Sean$ hadoop fs -du -s -h /wordcount/
86  /wordcount</p>

<p>Most commands print help when invoked w/o parameters.
localhost:mapreduce Sean$ hadoop fs -du -s -h hdfs://localhost:9000/*
15  hdfs://localhost:9000/hello2019.sh
4.7 M  hdfs://localhost:9000/tmp
266.0 K  hdfs://localhost:9000/user
86  hdfs://localhost:9000/wordcount</p></li>

<li><p><code>count</code><br />
统计一个目录下的文件数目. <code>hadoop fs -count /aaa/</code></p></li>

<li><p><code>setrep</code><br />
设置文件的副本数目. <code>replication</code>.</p>

<p>localhost:mapreduce Sean$ hadoop fs -setrep 3 /wordcount/input/hello2019.sh
Replication 3 set: /wordcount/input/hello2019.sh</p></li>
</ul>

<p><a href="https://img.it610.com/image/info8/ec6d459885f64f16b88f224fd0742ebd.jpg"><img src="https://img.it610.com/image/info8/ec6d459885f64f16b88f224fd0742ebd.jpg" alt="Hadoop Shell 命令 与
WordCount_第2张图片" /></a><a href="https://img.it610.com/image/info8/a021c27c2f864b8b94854eb2ba0bcf81.jpg"><img src="https://img.it610.com/image/info8/a021c27c2f864b8b94854eb2ba0bcf81.jpg" alt="Hadoop
Shell 命令 与
WordCount_第3张图片" /></a><br />
如果结点为3个, 但是设置为10个的时候.并不会设置10个,
这个是<code>namenode</code>中的原数据的副本数目,但是不一定是真实的副本数目(<code>视datanode的数目而定</code>).</p>

<hr />

<h3 id="wordcount">WordCount</h3>

<pre><code>localhost:mapreduce Sean$ hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wordcount/input/ /wordcount/output

localhost:mapreduce Sean$ hadoop jar hadoop-mapreduce-examples-2.7.5.jar wordcount /wordcount/input/ /wordcount/output
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
19/03/30 16:43:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
19/03/30 16:43:31 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
19/03/30 16:43:32 INFO input.FileInputFormat: Total input paths to process : 1
19/03/30 16:43:32 INFO mapreduce.JobSubmitter: number of splits:1
19/03/30 16:43:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1553933297569_0001
19/03/30 16:43:33 INFO impl.YarnClientImpl: Submitted application application_1553933297569_0001
19/03/30 16:43:33 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1553933297569_0001/
19/03/30 16:43:33 INFO mapreduce.Job: Running job: job_1553933297569_0001
19/03/30 16:43:43 INFO mapreduce.Job: Job job_1553933297569_0001 running in uber mode : false
19/03/30 16:43:43 INFO mapreduce.Job:  map 0% reduce 0%
19/03/30 16:43:48 INFO mapreduce.Job:  map 100% reduce 0%
19/03/30 16:43:54 INFO mapreduce.Job:  map 100% reduce 100%
19/03/30 16:43:54 INFO mapreduce.Job: Job job_1553933297569_0001 completed successfully
19/03/30 16:43:54 INFO mapreduce.Job: Counters: 49
    File System Counters
        FILE: Number of bytes read=74
        FILE: Number of bytes written=243693
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=157
        HDFS: Number of bytes written=44
        HDFS: Number of read operations=6
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
    Job Counters
        Launched map tasks=1
        Launched reduce tasks=1
        Data-local map tasks=1
        Total time spent by all maps in occupied slots (ms)=3271
        Total time spent by all reduces in occupied slots (ms)=3441
        Total time spent by all map tasks (ms)=3271
        Total time spent by all reduce tasks (ms)=3441
        Total vcore-milliseconds taken by all map tasks=3271
        Total vcore-milliseconds taken by all reduce tasks=3441
        Total megabyte-milliseconds taken by all map tasks=3349504
        Total megabyte-milliseconds taken by all reduce tasks=3523584
    Map-Reduce Framework
        Map input records=7
        Map output records=8
        Map output bytes=74
        Map output materialized bytes=74
        Input split bytes=115
        Combine input records=8
        Combine output records=6
        Reduce input groups=6
        Reduce shuffle bytes=74
        Reduce input records=6
        Reduce output records=6
        Spilled Records=12
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=123
        CPU time spent (ms)=0
        Physical memory (bytes) snapshot=0
        Virtual memory (bytes) snapshot=0
        Total committed heap usage (bytes)=306184192
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters
        Bytes Read=42
    File Output Format Counters
        Bytes Written=44
</code></pre>

<p><a href="https://img.it610.com/image/info8/2854825ab7c1424b81390d1ea9aeded7.jpg"><img src="https://img.it610.com/image/info8/2854825ab7c1424b81390d1ea9aeded7.jpg" alt="Hadoop Shell 命令 与
WordCount_第4张图片" /></a><br />
输出目录不能有其他内容, 否则会被进行覆盖.本次测试后的输出目录下出现2个文件:<code>success / part-r-0000</code>. 第一个文件为输出标准,
第二个文件是真实的文件.</p>

<pre><code>localhost:mapreduce Sean$ hadoop fs -cat /wordcount/output/part-r-00000
2019    1
able    1
cat 2
hello   1
kitty   1
pitty   2
</code></pre>

<p><a href="https://img.it610.com/image/info8/0eb2cc25053c472a96e07e86c0f6a438.jpg"><img src="https://img.it610.com/image/info8/0eb2cc25053c472a96e07e86c0f6a438.jpg" alt="Hadoop Shell 命令 与
WordCount_第5张图片" /></a></p>

<pre><code>localhost:mapreduce Sean$ ls
hadoop-mapreduce-client-app-2.7.5.jar           hadoop-mapreduce-client-hs-plugins-2.7.5.jar        hadoop-mapreduce-examples-2.7.5.jar
hadoop-mapreduce-client-common-2.7.5.jar        hadoop-mapreduce-client-jobclient-2.7.5-tests.jar   lib
hadoop-mapreduce-client-core-2.7.5.jar          hadoop-mapreduce-client-jobclient-2.7.5.jar     lib-examples
hadoop-mapreduce-client-hs-2.7.5.jar            hadoop-mapreduce-client-shuffle-2.7.5.jar       sources
</code></pre>

<p>这个文件夹下方有非常多的测试例子. 可以自己研究.</p>

<hr />

<h3 id="基本原理-大体">基本原理(大体)</h3>

<p>存储在HDFS内的文件其实还是存储在本地的, 只是它是一个分布式的文件系统而已. 我们看下我们之前存储的.</p>

<p>注意,由于我本地的是<code>DataNode</code>与<code>NameNode</code>安装在一起的, 所以文件目录下结点如下所示:</p>

<pre><code>localhost:tmp Sean$ tree
.
├── dfs
│   ├── data
│   │   ├── current
│   │   │   ├── BP-586017156-127.0.0.1-1553485799471
│   │   │   │   ├── current
│   │   │   │   │   ├── VERSION
│   │   │   │   │   ├── dfsUsed
│   │   │   │   │   ├── finalized
│   │   │   │   │   │   └── subdir0
│   │   │   │   │   │       └── subdir0
│   │   │   │   │   │           ├── blk_1073741825
│   │   │   │   │   │           ├── blk_1073741825_1001.meta
│   │   │   │   │   └── rbw
│   │   │   │   ├── scanner.cursor
│   │   │   │   └── tmp
│   │   │   └── VERSION
│   │   └── in_use.lock
│   ├── name
│   │   ├── current
│   │   │   ├── VERSION
│   │   │   ├── edits_0000000000000000001-0000000000000000118
│   │   │   ├── edits_inprogress_0000000000000001233
│   │   │   ├── fsimage_0000000000000001230
│   │   │   ├── fsimage_0000000000000001230.md5
│   │   │   ├── fsimage_0000000000000001232
│   │   │   ├── fsimage_0000000000000001232.md5
│   │   │   └── seen_txid
│   │   └── in_use.lock
│   └── namesecondary
│       ├── current
│       │   ├── VERSION
│       │   ├── edits_0000000000000000001-0000000000000000118
│       │   ├── edits_0000000000000000119-0000000000000000943
│       │   ├── edits_0000000000000001231-0000000000000001232
│       │   ├── fsimage_0000000000000001230
│       │   ├── fsimage_0000000000000001230.md5
│       │   ├── fsimage_0000000000000001232
│       │   └── fsimage_0000000000000001232.md5
│       └── in_use.lock
└── nm-local-dir
    ├── filecache
    ├── nmPrivate
    └── usercache
</code></pre>

<ul>
<li>NameNode: 文件的管理</li>
<li>DataNode: 文件的切分与管理(Socket / Netty)</li>
</ul>

<hr />

<h3 id="hdfs命令">HDFS命令</h3>

<ul>
<li><p><code>hdfs dfsadmin -report</code> 查看集群状态</p>

<p>localhost:Desktop Sean$ hdfs dfsadmin -report
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
19/04/03 15:51:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform&hellip; using builtin-java classes where applicable
Configured Capacity: 500068036608 (465.72 GB)
Present Capacity: 182055092460 (169.55 GB)
DFS Remaining: 182048903168 (169.55 GB)
DFS Used: 6189292 (5.90 MB)
DFS Used%: 0.00%
Under replicated blocks: 27
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0</p>

<hr />

<p>Live datanodes (1):</p>

<p>Name: 127.0.0.1:50010 (localhost)
Hostname: localhost
Decommission Status : Normal
Configured Capacity: 500068036608 (465.72 GB)
DFS Used: 6189292 (5.90 MB)
Non DFS Used: 313001643796 (291.51 GB)
DFS Remaining: 182048903168 (169.55 GB)
DFS Used%: 0.00%
DFS Remaining%: 36.40%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Wed Apr 03 15:51:21 CST 2019</p></li>
</ul>

<hr />

<h3 id="reference">Reference</h3>

<p>[1].Hadoop Shell命令<br />
[2]. 介绍hadoop中的hadoop和hdfs命令<br />
[3]. Hadoop学习笔记4之HDFS常用命令</p>

        </div>

        


        

<div class="post-archive">
    <h2>See Also</h2>
    <ul class="listing">
        
        <li><a href="/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a></li>
        
        <li><a href="/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/">009Shell脚本下条件测试eqne</a></li>
        
        <li><a href="/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/">00Pythonmanagepyshell和Python的分析</a></li>
        
        <li><a href="/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a></li>
        
        <li><a href="/posts/018dockerfileshell/">018DockerfileSHELL</a></li>
        
    </ul>
</div>


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='https://zaina.newban.cn/tags/shell'>shell</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
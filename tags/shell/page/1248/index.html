<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>shell | 开发者问答集锦</title>
    <meta property="og:title" content="shell - 开发者问答集锦">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="">
    <meta name="description" content="shell">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/tags/shell/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <link rel="alternate" type="application/rss+xml+xml" href="https://zaina.newban.cn/tags/shell/index.xml" title="开发者问答集锦" />
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <h3 class="archive-title">
        包含标签
        <span class="keyword">shell</span>
        的文章
    </h3>
    

    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%90%AF%E5%8A%A8%E7%9A%84%E6%97%B6%E5%80%99%E6%8A%A5%E9%94%99errorsparksparkcontexterrorinitializingsparkcontextconnectexception/">sparkShell启动的时候报错ERRORsparkSparkContextErrorinitializingSparkContextConnectException</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            spark Shell启动的时候报错：ERROR spark.SparkContext: Error initializing SparkContext.ConnectException: Call From hadoop202/192.168.1.202 to hadoop202:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:
spark连接HDFS拒绝连接，其实是犯了一个很低级的错误：
hadoop的hdfs-site.xml里面配置的HDFS集群端口是8020，在spark-defaults.conf和spark- env.sh写的却是9000，看似简单的问题对于新手来说却不容易发现。
具体报错信息如下：
ERROR spark.SparkContext: Error initializing SparkContext. java.net.ConnectException: Call From hadoop202/192.168.1.202 to hadoop202:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792) at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732) at org.apache.hadoop.ipc.Client.call(Client.java:1479) at org.apache.hadoop.ipc.Client.call(Client.java:1412) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229) at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source) at org.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%90%AF%E5%8A%A8%E7%9A%84%E6%97%B6%E5%80%99%E6%8A%A5%E9%94%99errorsparksparkcontexterrorinitializingsparkcontextconnectexception/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%92%8Cscala%E9%94%99%E8%AF%AF/">sparkshell和scala错误</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            ……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%92%8Cscala%E9%94%99%E8%AF%AF/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%9C%A8yarn%E4%B8%8A%E5%8D%96%E5%BC%84%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8A%A5%E9%94%99thespecifieddatastoredrivercommysqljdbcdriverwasnotfound/">sparkshell在yarn上卖弄启动时报错ThespecifieddatastoredrivercommysqljdbcDriverwasnotfound</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            在安装好的Hadoop集群和spark集群中安装好hive。但是在利用yarn启动spark-shell时候，报了以下错误：
The specified datastore driver (&ldquo;com.mysql.jdbc.Driver&rdquo;) was not found 。。。。。
这个时候与找不到了MySQL的驱动，所以需要在spark-defaults.conf中进行指定配置。
spark.executor.extraClassPath /home/hadoop/jars/mysql-connector-java-5.1.46-bin.jar spark.driver.extraClassPath /home/hadoop/jars/mysql-connector-java-5.1.46-bin.jar  然后启动即可：
[bing@hadoop102 spark]$ bin/spark-shell &ndash;master yarn
启动成功：……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%9C%A8yarn%E4%B8%8A%E5%8D%96%E5%BC%84%E5%90%AF%E5%8A%A8%E6%97%B6%E6%8A%A5%E9%94%99thespecifieddatastoredrivercommysqljdbcdriverwasnotfound/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/">sparkshell基本用法</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            spark-shell 是 scala 语言的 REPL（Read-Eval-Print-Loop，通俗地理解就是命令行模式） 环境，同时针对 spark 做了一些拓展。
 退出 spark-shell：scala&gt; :quit  0. 常见参数  --master spark://xx:7077：指定master节点； --executor-memory 512m：每一个执行节点所需的内存； --total-executor-cores 2：集群用到的 CPU 核数；  1. 启动 spark-shell 的方法  本机
$ spark-shell --master local[N]  $ spark-shell &ndash;master local[*]
  通过设定local[N]参数来启动本地 Spark 集群，其中 N 表示运行的线程数，或者用 * 表示使用机器上所有可用的核数。
在本地模式设定内存，如设置本地进程使用 2GB 内存。
 $ spark-shell --driver-memory 2g --master local[*]   启动在 hdfs 上
 在 yarn 上启动
  如果你有一个 hadoop 集群，并且 hadoop 版本支持 yarn，通过为 Spark master 设定 yarn-client 参数值，便可在集群上启动 spark 作业：……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/">sparkshell基础操作持续更新</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            1.概述
Spark SQL 是 Spark 处理结构化数据的一个模块。与基础的 Spark RDD API 不同，Spark SQL 提供了 查询结构化数据 及 计算结果 等信息的接口。在内部，Spark SQL 使用这个额外的信息去执行额外的优化。有几种方式可以跟 Spark SQL 进行交互，包括 SQL 和 Dataset API。当使用相同执行引擎进行计算时，无论使用哪种 API / 语言都可以快速的计算。这种统一意味着开发人员能够在基于提供最自然的方式来表达一个给定的 transformation API 之间实现轻松的来回切换不同的 。
2.SQL
Spark SQL 的功能之一是执行 SQL 查询。Spark SQL 也能够被用于从已存在的 Hive 环境中读取数据，不过需要进行hive配置（关于这部分不再我们讨论范围内，请自行百度）。Spark SQL也可以直接从本地读取文件。
3.Datasets 和 DataFrames
一个 Dataset 是一个分布式的数据集合。Dataset 是在 Spark 1.6 中被添加的新接口，它提供了 RDD 的优点（强类型化，能够使用强大的 lambda 函数）与 Spark SQL 优化的执行引擎的好处。一个 Dataset 可以从 JVM 对象来构造并且使用转换功能（map，flatMap，filter，等等）。Dataset API 在 Scala 和 Java 中是可用的。Python 不支持 Dataset API。但是由于 Python 的动态特性，许多 Dataset API 的有点已经可用了（也就是说，你可能通过 name 天生的 row.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%A6%82%E4%BD%95%E7%B2%98%E8%B4%B4%E6%8D%A2%E8%A1%8C%E4%BB%A3%E7%A0%81/">sparkshell如何粘贴换行代码</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            前言 平时经常性的有一些临时统计数据需求，用hive虽然很方便，但是等待时间有点长，spark- shell成为了我常用的一种方式，不过，一般我都是在IDEA把代码写好，然后复制到spark-shell上面，这个时候就会出现如下问题：
比如我复制如下代码（ 有换行 ）：
sql(sqlText = &quot;select statis_month,count(*) from ods.ods_app_score_m where statis_month &quot; + &quot;like '2019%' group by statis_month&quot;) .show()  spark-shell中竟然显示如下：
scala&gt; sql(sqlText = &quot;select statis_month,count(*) from ods.ods_app_score_m where statis_month &quot; + | &quot;like '2019%' group by statis_month&quot;) res0: org.apache.spark.sql.DataFrame = [statis_month: string, count(1): bigint] scala&gt; .show()  显然sql中的部分，它自动识别了换行，dataset的算子部分它并没有识别出来，就默认为是另一段代码了
如何解决 直接看操作
scala&gt; :paste // Entering paste mode (ctrl-D to finish) sql(sqlText = &quot;select statis_month,count(*) from ods.ods_app_score_m where statis_month &quot; + &quot;like '2019%' group by statis_month&quot;) .……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%A6%82%E4%BD%95%E7%B2%98%E8%B4%B4%E6%8D%A2%E8%A1%8C%E4%BB%A3%E7%A0%81/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%AE%9E%E7%8E%B0pagerank/">sparkshell实现PageRank</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            ……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%AE%9E%E7%8E%B0pagerank/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E5%AE%9E%E9%AA%8C1%E7%AE%80%E5%8D%95%E7%9A%84shell%E6%93%8D%E4%BD%9C/">Sparkshell实验1简单的shell操作</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            某大学计算机系的成绩，数据格式如下所示：
Tom,DataBase,80
Tom,Algorithm,50
Tom,DataStructure,60
Jim,DataBase,90
Jim,Algorithm,60
Jim,DataStructure,80
……
请根据给定的实验数据，在 spark-shell 中通过编程来计算以下内容：
（1）该系总共有多少学生；
val lines=sc.textFile(&quot;/test/Data1.txt&quot;)//打开文件 val par=lines.map(row=&gt;row.split(&quot;,&quot;)(0))//切分取第一数值 val distinct_par=par.distinct()//去重 distinct_par.count//输出  （2）Tom 同学的总成绩平均分是多少；
val lines=sc.textFile(&quot;/test/Data1.txt&quot;)//打开文件 val pare=lines.filter(row=&gt;row.split(&quot;,&quot;)(0)==&quot;Tom&quot;)//fileter pare.foreach(println)//输出内容 /*Tom,DataBase,26 Tom,Algorithm,12 Tom,OperatingSystem,16 Tom,Python,40 Tom,Software,60*/ pare.map(row=&gt;(row.split(&quot;,&quot;)(0),row.split(&quot;,&quot;)(2).toInt)).mapValues(x=&gt;(x,1)).reduceByKey((x,y)=&gt;(x._1+y._1,x._2+y._2)).mapValues(x=&gt;(x._1/x._2)).collect() //res13: Array[(String, Int)] = Array((Tom,30))  （4）求每名同学的选修的课程门数；
val lines=sc.textFile(&quot;/test/Data1.txt&quot;) val pare=lines.map(row=&gt;(row.spilt(&quot;,&quot;)(0),row.split(&quot;,&quot;)(1))) pare.mapValues(x=&gt;(x,1)).reduceByKey((x,y)=&gt;(&quot; &quot;,x._2+y._2)).mapValues(x =&gt;x._2).foreach(println)  （5）该系 DataBase 课程共有多少人选修；
val pare=lines.filter(row=&gt;row.split(&quot;,&quot;)(1)==&quot;DataBase&quot;) pare.count  （6）各门课程的平均分是多少；
val par=lines.map(row=&gt;(row.split(&quot;,&quot;)(1),row.split(&quot;,&quot;)(2).toInt)) par.mapValues(x=&gt;(x,1)).reduceByKey((x,y)=&gt;(x._1+y._1,x._2+y._2)).mapValues(x=&gt;(x._1/x._2)).collect()  （7）使用累加器计算共有多少人选了 DataBase 这门课
val pare=lines.filter(row=&gt;row.split(&quot;,&quot;)(1)==&quot;DataBase&quot;).map(row=&gt;(row.split(&quot;,&quot;)(1),1)) val accum=sc.longAccumulator(&quot;My Accumulator&quot;) pare.values.foreach(x=&gt;accum.add(x)) accum.value  转载于:https://www.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E5%AE%9E%E9%AA%8C1%E7%AE%80%E5%8D%95%E7%9A%84shell%E6%93%8D%E4%BD%9C/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E6%8F%90%E4%BA%A4/">sparkshell提交</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            spark-shell（REPL） （1.）直接运行spark-shell启动的是本地的
命令：
[root@bigdata111 ~]#spark-shell  Spark context available as &lsquo;sc&rsquo; (master = local[*], app id = local-1577740473039).
scala&gt; sc.textFile(&quot;/opt/module/test/one&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect res0: Array[(String, Int)] = Array((is,2), (you,1), (plus,1), (name,2), (hadoop,1), (hi,1), (jh,1), (do,1), (HP,1), (hello,1), (java,2), (my,2))  （2.）运行spark-shell &ndash;master spark://bigdata111:7077 启动集群模式的spark-shell
命令：
[root@bigdata111 ~]#spark-shell --master spark://bigdata111:7077  Spark context available as &lsquo;sc&rsquo; (master = spark://bigdata111:7077, app id = app-20191231051535-0002)
scala&gt; sc.textFile(&quot;hdfs://bigdata111:9000/one&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect res0: Array[(String, Int)] = Array((is,2), (plus,1), (jh,1), (HP,1), (hello,1), (java,2), (my,2), (you,1), (name,2), (hadoop,1), (hi,1), (do,1)) scala&gt; sc.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E6%8F%90%E4%BA%A4/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E8%AF%BB%E6%88%90%E8%A1%A8%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84hdfsdfsls/">sparkshell数据文件读成表的两种方式相对路径hdfsdfsls</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            park SQL应用 Spark Shell启动后，就可以用Spark SQL API执行数据分析查询。 在第一个示例中，我们将从文本文件中加载用户数据并从数据集中创建一个DataFrame对象。然后运行DataFrame函数，执行特定的数据选择查询。 文本文件customers.txt中的内容如下： 100, John Smith, Austin, TX, 78727 200, Joe Johnson, Dallas, TX, 75201 300, Bob Jones, Houston, TX, 77028 400, Andy Davis, San Antonio, TX, 78227 500, James Williams, Austin, TX, 78727 下述代码片段展示了可以在Spark Shell终端执行的Spark SQL命令。 // 首先用已有的Spark Context对象创建SQLContext对象 val sqlContext = new org.apache.spark.sql.SQLContext(sc); // 导入语句，可以隐式地将RDD转化成DataFrame import sqlContext.implicits._ // 创建一个表示客户的自定义类 case class Customer(customer_id: Int, name: String, city: String, state: String, zip_code: String) // 用数据集文本文件创建一个Customer对象的DataFrame val dfCustomers = sc.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E8%AF%BB%E6%88%90%E8%A1%A8%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84hdfsdfsls/">阅读全文</a></p>
        </div>
    </article>
    

    



<ol class="page-navigator">
    
    <li class="prev">
        <a href="https://zaina.newban.cn/tags/shell/page/1247/">上一页</a>
    </li>
    

    

    
        
        
    
    

    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/">1</a>
        </li>
        
    
        
        <li>
            <span>...</span>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1246/">1246</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1247/">1247</a>
        </li>
        
    
        
        
        <li  class="current">
            <a href="https://zaina.newban.cn/tags/shell/page/1248/">1248</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1249/">1249</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1250/">1250</a>
        </li>
        
    
        
        <li>
            <span>...</span>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1621/">1621</a>
        </li>
        
    

    
    

    <li class="next">
        <a href="https://zaina.newban.cn/tags/shell/page/1249/">下一页</a>
    </li>
    
</ol>




</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>



<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>
<!doctype html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>shell | 开发者问答集锦</title>
    <meta property="og:title" content="shell - 开发者问答集锦">
    <meta property="og:type" content="article">
        
        
    <meta name="Keywords" content="">
    <meta name="description" content="shell">
        
    <meta name="author" content="">
    <meta property="og:url" content="https://zaina.newban.cn/tags/shell/">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <link rel="alternate" type="application/rss+xml+xml" href="https://zaina.newban.cn/tags/shell/index.xml" title="开发者问答集锦" />
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://zaina.newban.cn">
                        开发者问答集锦
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="" href="https://zaina.newban.cn">首页</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <h3 class="archive-title">
        包含标签
        <span class="keyword">shell</span>
        的文章
    </h3>
    

    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkshell%E9%80%80%E5%87%BA%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/">Sparkshell退出操作以及出现问题的解决方法</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            启动spark的操作是在其根目录下输入，在终端中输入：
 ./bin/spark-shell  退出的正确操作是：
:quit  然而我们的错误操作是：
Ctrl+C或Z  这样就会在重启的时候报错。
wugaosheng:spark-2.2.0-bin-hadoop2.7 eric$ ./bin/spark-shell Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 17/10/06 17:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 17/10/06 17:15:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041. 17/10/06 17:15:33 ERROR Schema: Failed initialising database.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkshell%E9%80%80%E5%87%BA%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E5%87%BA%E7%8E%B0%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksparkshell%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/">Sparksparkshell命令使用</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            环境： 操作系统：CentOS7.3
Java: jdk1.8.0_45
Hadoop：hadoop-2.6.0-cdh5.14.0.tar.gz
1. spark-shell 使用帮助
[hadoop@hadoop01 ~]$ cd app/spark-2.2.0-bin-2.6.0-cdh5.7.0/bin [hadoop@hadoop01 bin]$ ./spark-shell --help Usage: ./bin/spark-shell [options] # 重要参数 Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, or local. --name NAME A name of your application. --jars JARS Comma-separated list of local jars to include on the driver and executor classpaths. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --conf PROP=VALUE Arbitrary Spark configuration property.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksparkshell%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksparkshell%E6%93%8D%E4%BD%9C/">Sparksparkshell操作</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            RDD(Resilient Distributed Datasets),弹性分布式数据集,是分布式内存的一个抽象概念，RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，只能通过在其他RDD执行确定的转换操作（如map、join和group by）而创建，然而这些限制使得实现容错的开销很低.
创建RDD的两种方法:
1.并行化集合
val data= sc.parallelize(Array(1,2,3))
2.外部数据集
val textFile = sc.textFile(&ldquo;file:///opt/word.txt&rdquo;)
map(对所有的元素进行操作)函数:

filter(过滤元素)函数:

count(计算元素个数)函数:

distinct(去重):

union(并集):

intersection(交集):

cartesian(笛卡尔积):

sortByKey(排序)函数:

groupByKey(分组)函数:

reduceByKey数据聚合函数:

cogroup

first(第一个元素):

take(返回前几个元素):

在/opt目录下新建文件word.txt,并输入一些内容。
启动spark-shell
val textFile = sc.textFile(&ldquo;file:///opt/word.txt&rdquo;)
textFile.first()
val wordCount = textFile.flatMap(line =&gt; line.split(&rdquo; &ldquo;)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)
wordCount.collect()

val rr=sc.textFile(&ldquo;file:///opt/word.txt&rdquo;).flatMap(x=&gt;x.split(&rdquo; &ldquo;)).countByValue()

上面是读取本地文件，还可以读取hdfs上面的文件……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksparkshell%E6%93%8D%E4%BD%9C/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparkspawnaappviasparkshellvssparksubmit/">sparkspawnaappviasparkshellVSsparksubmit</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            ……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparkspawnaappviasparkshellvssparksubmit/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksql%E4%BB%8Emysql%E4%B8%AD%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E5%B0%86%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0mysql%E4%B8%ADsparkshell%E6%96%B9%E5%BC%8Fsparksql%E7%A8%8B%E5%BA%8F/">SparkSQL从MySQL中加载数据以及将数据写入到mysql中SparkShell方式SparkSQL程序</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            分享一下我老师大神的人工智能教程！零基础，通俗易懂！http://blog.csdn.net/jiangjunshow
也欢迎大家转载本篇文章。分享知识，造福人民，实现我们中华民族伟大复兴！
1． JDBC Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。
1.1． 从MySQL中加载数据（Spark Shell方式） 1.启动Spark Shell，必须指定mysql连接驱动jar包
[root@hadoop1 spark-2.1.1-bin-hadoop2.7]# bin/spark-shell --master spark://hadoop1:7077,hadoop2:7077 --jars /home/tuzq/software/spark-2.1.1-bin-hadoop2.7/jars/mysql-connector-java-5.1.38.jar --driver-class-path /home/tuzq/software/spark-2.1.1-bin-hadoop2.7/jars/mysql-connector-java-5.1.38.jar   1  
2.从mysql中加载数据
进入bigdata中创建person表：
CREATE DATABASE bigdata CHARACTER SET utf8;USE bigdata;CREATE TABLE person ( id INT(10) AUTO_INCREMENT PRIMARY KEY, name varchar(100), age INT(3)) ENGINE=INNODB DEFAULT CHARSET=utf8;   1 2 3 4 5 6 7  并初始化数据：

scala&gt; val sqlContext = new org.apache.spark.sql.SQLContext(sc)   1……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksql%E4%BB%8Emysql%E4%B8%AD%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E5%B0%86%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E5%88%B0mysql%E4%B8%ADsparkshell%E6%96%B9%E5%BC%8Fsparksql%E7%A8%8B%E5%BA%8F/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksql%E6%95%B4%E5%90%88hive%E5%9C%A8sparksql%E5%91%BD%E4%BB%A4%E5%92%8Csparkshell%E5%91%BD%E4%BB%A4%E4%B8%8B%E6%89%A7%E8%A1%8Csql%E5%91%BD%E4%BB%A4%E5%92%8C%E6%95%B4%E5%90%88%E8%B0%83%E7%94%A8hive/">SparkSql整合hive在sparksql命令和sparkshell命令下执行sql命令和整合调用hive</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            1.安装hive
如果想创建一个数据库用户，并且为数据库赋值权限，可以参考：http://blog.csdn.net/tototuzuoquan/article/details/52785504
2.将配置好的hive-site.xml、core-site.xml、hdfs-site.xml放入$SPARK_HOME/conf目录下
[root@hadoop1 conf]# cd /home/tuzq/software/hive/apache-hive-1.2.1-bin [root@hadoop1 conf]# cp hive-site.xml $SPARK_HOME/conf [root@hadoop1 spark-1.6.2-bin-hadoop2.6]# cd $HADOOP_HOME [root@hadoop1 hadoop]# cp core-site.xml $SPARK_HOME/conf [root@hadoop1 hadoop]# cp hdfs-site.xml $SPARK_HOME/conf 同步spark集群中的conf中的配置 [root@hadoop1 conf]# scp -r * root@hadoop2:$PWD [root@hadoop1 conf]# scp -r * root@hadoop3:$PWD [root@hadoop1 conf]# scp -r * root@hadoop4:$PWD [root@hadoop1 conf]# scp -r * root@hadoop5:$PWD  放入进去之后，注意重新启动Spark集群，关于集群启动和停止，可以参考：
http://blog.csdn.net/tototuzuoquan/article/details/74481570  修改spark的log4j打印输出的日志错误级别为Error。修改内容为：

3.启动spark-shell时指定mysql连接驱动位置
bin/spark-shell --master spark://hadoop1:7077,hadoop2:7077 --executor-memory 1g --total-executor-cores 2 --driver-class-path /home/tuzq/software/spark-1.6.2-bin-hadoop2.6/lib/mysql-connector-java-5.1.38.jar  如果启动的过程中报如下错：……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksql%E6%95%B4%E5%90%88hive%E5%9C%A8sparksql%E5%91%BD%E4%BB%A4%E5%92%8Csparkshell%E5%91%BD%E4%BB%A4%E4%B8%8B%E6%89%A7%E8%A1%8Csql%E5%91%BD%E4%BB%A4%E5%92%8C%E6%95%B4%E5%90%88%E8%B0%83%E7%94%A8hive/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksubmit%E5%92%8Csparkshell%E5%90%8E%E9%9D%A2%E5%8F%AF%E8%B7%9F%E7%9A%84%E5%8F%82%E6%95%B0/">sparksubmit和sparkshell后面可跟的参数</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            站在巨人的肩膀上：http://spark.apache.org/docs/latest/submitting-applications.html Submitting Applications The spark-submit script in Spark’s bin directory is used to launch applications on a cluster. It can use all of Spark’s supported cluster managersthrough a uniform interface so you don’t have to configure your application specially for each one.
Bundling Your Application’s Dependencies If your code depends on other projects, you will need to package them alongside your application in order to distribute the code to a Spark cluster.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksubmit%E5%92%8Csparkshell%E5%90%8E%E9%9D%A2%E5%8F%AF%E8%B7%9F%E7%9A%84%E5%8F%82%E6%95%B0/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/sparksubmit%E6%8F%90%E4%BA%A4%E7%9A%84shell%E8%84%9A%E6%9C%AC/">sparksubmit提交的shell脚本</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            spark-submit向yarn提交application的脚本，包括spark参数、环境变量、应用程序参数传入
#!/bin/bash source ~/.bash_profile APP_HOME=/home/data_user/recommend echo $APP_HOME ###################### etl候选集和浏览数据集 ############################################## dt=`date -d &quot;now 1 days ago &quot; &quot;+%Y%m%d&quot;` # 用户商品候选集 hive -hiveconf dt=&quot;${dt}&quot; -f $APP_HOME/hql/tmp.rdm_user_product_candidate.sql # 用户历史浏览商品 hive -hiveconf dt=&quot;${dt}&quot; -f $APP_HOME/hql/tmp.rdm_user_rating_product.sql ################################# spark job ########################################### for f in $APP_HOME/lib/*.jar; do app_CLASSPATH=$f,${app_CLASSPATH} done len=${#app_CLASSPATH}-1 JAR_PATH=${app_CLASSPATH:0:len} # 依赖jar包 rawRatingSQLFile=$APP_HOME/conf/user_product_rating.sql # 用户商品评分数据 userProdSQLFile=$APP_HOME/conf/user_product_candidate.sql # 用户商品候选数据 productType=phone # 商品类型 saveTable=edw.rdm_cb_item_user_phone # 推荐结果表 partitionNum=256 # 数据集分区数 topN=10 # 推荐结果取topN数 spark-submit --master yarn \ --deploy-mode client \ --num-executors 16 \ --executor-cores 2 \ --executor-memory 8g \ --class com.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/sparksubmit%E6%8F%90%E4%BA%A4%E7%9A%84shell%E8%84%9A%E6%9C%AC/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/spark%E4%B9%8Bhivesupport%E8%BF%9E%E6%8E%A5sparkshell%E5%92%8Cidea/">Spark之HiveSupport连接sparkshell和IDEA</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            ……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/spark%E4%B9%8Bhivesupport%E8%BF%9E%E6%8E%A5sparkshell%E5%92%8Cidea/">阅读全文</a></p>
        </div>
    </article>
    
    <article class="post">
        <header>
            <h1 class="post-title">
                <a href="https://zaina.newban.cn/posts/spark%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%E4%BA%8Cspark%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8Fsparkshellsparksubmit%E5%90%84%E7%A7%8D%E7%89%88%E6%9C%AC%E7%9A%84wordcount/">Spark从入门到精通二spark任务的提交方式sparkshellsparksubmit各种版本的wordcount</a>
            </h1>
        </header>
        <date class="post-meta meta-date">
            2020年9月2日
        </date>
        
        <div class="post-content">
            版权声明：本文为博主原创文章，未经博主允许不得转载!!
欢迎访问:https://blog.csdn.net/qq_21439395/article/details/82779266
交流QQ: 824203453
 执行Spark程序  使用spark-shell命令和spark-submit命令来提交spark任务。
当执行测试程序，使用spark-shell，spark的交互式命令行
提交spark程序到spark集群中运行时，spark-submit
 1. 执行第一个spark示例程序  spark-submit &ndash;class org.apache.spark.examples.SparkPi /root/apps/spark/examples/jars/spark-examples_2.11-2.2.0.jar 100
该算法是利用蒙特·卡罗算法求PI(圆周率)
spark任务提交的方式：
spark-submit &ndash;master spark://hdp-01:7077 &ndash;class xxx.SparkPi /root/xx.jar 输入输出参数
怎么用：
提交正式任务，或者有jar包，使用spark-submit ；本地测试，选用spark-shell
 1. 启动Spark Shell  spark-shell 用命令行的方式提交任务到集群的一个客户端。spark- shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序。
 1. 1. 启动spark shell  直接启动spark-shell默认使用的是local模式，和spark集群无关
只要把spark安装包解压了，就可以运行local模式

local模式没有指定master地址，仅在本机启动一个进程（SparkSubmit），没有与集群建立联系。但是也可以正常启动spark shell和执行spark shell中的程序
指定集群模式启动：
hdfs://hdp-01:9000
spark的协议URI： spark://hdp-01:7077
spark-shell &ndash;master 
在webUI界面，可以查看到正在运行的程序：

Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可
 1. 1. 在spark shell中编写WordCount程序 首先启动hdfs 向hdfs上传一个文件到hdfs://hdp-01:9000/wordcount/input/a.txt 在spark shell中用scala语言编写spark程序  scala&gt; sc.……
            <p class="readmore"><a href="https://zaina.newban.cn/posts/spark%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%E4%BA%8Cspark%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%8F%90%E4%BA%A4%E6%96%B9%E5%BC%8Fsparkshellsparksubmit%E5%90%84%E7%A7%8D%E7%89%88%E6%9C%AC%E7%9A%84wordcount/">阅读全文</a></p>
        </div>
    </article>
    

    



<ol class="page-navigator">
    
    <li class="prev">
        <a href="https://zaina.newban.cn/tags/shell/page/1249/">上一页</a>
    </li>
    

    

    
        
        
    
    

    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/">1</a>
        </li>
        
    
        
        <li>
            <span>...</span>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1248/">1248</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1249/">1249</a>
        </li>
        
    
        
        
        <li  class="current">
            <a href="https://zaina.newban.cn/tags/shell/page/1250/">1250</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1251/">1251</a>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1252/">1252</a>
        </li>
        
    
        
        <li>
            <span>...</span>
        </li>
        
    
        
        
        <li >
            <a href="https://zaina.newban.cn/tags/shell/page/1621/">1621</a>
        </li>
        
    

    
    

    <li class="next">
        <a href="https://zaina.newban.cn/tags/shell/page/1251/">下一页</a>
    </li>
    
</ol>




</div>

                    <footer id="footer">
    <div>
        &copy; 2020 <a href="https://zaina.newban.cn">开发者问答集锦 By </a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://www.flysnow.org/" target="_black"><span class="badge-subject">Design by</span><span class="badge-value bg-brightgreen">飞雪无情</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>



<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://zaina.newban.cn/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://zaina.newban.cn">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://zaina.newban.cn/posts/001rubyruby%E4%B8%AD%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F%E5%AE%9E%E4%BE%8B%E5%8F%98%E9%87%8F%E5%B1%80%E9%83%A8%E5%8F%98%E9%87%8F%E7%B1%BB%E5%8F%98%E9%87%8Fsymbol%E5%AF%B9%E6%AF%94/" title="001rubyRuby中全局变量实例变量局部变量类变量Symbol对比">001rubyRuby中全局变量实例变量局部变量类变量Symbol对比</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/007hadoop%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AEhadoop%E9%9B%86%E7%BE%A4%E7%9A%84%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95ssh%E5%85%8D%E7%99%BB%E9%99%86%E9%85%8D%E7%BD%AEstartallshhdfs%E5%B8%B8%E7%94%A8%E7%9A%84shell/" title="007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell">007Hadoop集群配置Hadoop集群的启动和测试SSH免登陆配置startallshhdfs常用的shell</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/009shell%E8%84%9A%E6%9C%AC%E4%B8%8B%E6%9D%A1%E4%BB%B6%E6%B5%8B%E8%AF%95eqne/" title="009Shell脚本下条件测试eqne">009Shell脚本下条件测试eqne</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/00pythonmanagepyshell%E5%92%8Cpython%E7%9A%84%E5%88%86%E6%9E%90/" title="00Pythonmanagepyshell和Python的分析">00Pythonmanagepyshell和Python的分析</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/010zookeeper%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5zookeeper%E7%9A%84%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BAzookeeper%E7%9A%84shell%E5%91%BD%E4%BB%A4/" title="010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令">010Zookeeper的基本概念Zookeeper的集群搭建Zookeeper的shell命令</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/018dockerfileshell/" title="018DockerfileSHELL">018DockerfileSHELL</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%85%A5%E9%97%A801bashshell%E7%89%B9%E6%80%A7/" title="01Shell入门01bashShell特性">01Shell入门01bashShell特性</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%8F%98%E9%87%8F/" title="01Shell变量">01Shell变量</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E5%9F%BA%E7%A1%80%E6%A6%82%E8%BF%B0%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8Fbash%E5%9F%BA%E6%9C%AC%E5%8A%9F%E8%83%BD/" title="01Shell基础概述脚本执行方式Bash基本功能">01Shell基础概述脚本执行方式Bash基本功能</a>
    </li>
    
    <li>
        <a href="https://zaina.newban.cn/posts/01shell%E7%BC%96%E7%A8%8Bhelloworld/" title="01shell编程helloworld">01shell编程helloworld</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href="/categories">分类</a></h3>
<ul class="widget-list">
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href="/tags">标签</a></h3>
<div class="tagcloud">
    
    <a href="https://zaina.newban.cn/tags/ruby/">ruby</a>
    
    <a href="https://zaina.newban.cn/tags/shell/">shell</a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">其它</h3>
        <ul class="widget-list">
            <li><a href="https://zaina.newban.cn/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
</body>

</html>